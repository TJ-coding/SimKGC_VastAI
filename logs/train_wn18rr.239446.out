+ set -e
+ TASK=WN18RR
+++ dirname scripts/train_wn.sh
++ cd scripts
++ cd ..
++ pwd
+ DIR=/cl/work8/tomoyuki-j/Projects/simkgc/SimKGC
+ echo 'working directory: /cl/work8/tomoyuki-j/Projects/simkgc/SimKGC'
working directory: /cl/work8/tomoyuki-j/Projects/simkgc/SimKGC
+ '[' -z ./checkpoint/wn18rr/ ']'
+ '[' -z '' ']'
+ DATA_DIR=/cl/work8/tomoyuki-j/Projects/simkgc/SimKGC/data/WN18RR
+ python3 -u main.py --model-dir ./checkpoint/wn18rr/ --pretrained-model bert-base-uncased --pooling mean --lr 5e-5 --use-link-graph --train-path /cl/work8/tomoyuki-j/Projects/simkgc/SimKGC/data/WN18RR/train.txt.json --valid-path /cl/work8/tomoyuki-j/Projects/simkgc/SimKGC/data/WN18RR/valid.txt.json --task WN18RR --batch-size 1024 --print-freq 20 --additive-margin 0.02 --use-amp --use-self-negative --pre-batch 0 --finetune-t --epochs 50 --workers 4 --max-to-keep 3
[2025-02-05 13:13:43,386 INFO] Load 40559 entities from /cl/work8/tomoyuki-j/Projects/simkgc/SimKGC/data/WN18RR/entities.json
[2025-02-05 13:13:43,387 INFO] Triplets path: ['/cl/work8/tomoyuki-j/Projects/simkgc/SimKGC/data/WN18RR/train.txt.json']
[2025-02-05 13:13:43,594 INFO] Triplet statistics: 20 relations, 114240 triplets
[2025-02-05 13:13:43,594 INFO] Start to build link graph from /cl/work8/tomoyuki-j/Projects/simkgc/SimKGC/data/WN18RR/train.txt.json
[2025-02-05 13:13:43,835 INFO] Done build link graph with 39309 nodes
[2025-02-05 13:13:43,936 INFO] Use 4 gpus for training
[2025-02-05 13:13:44,239 INFO] Build tokenizer from bert-base-uncased
[2025-02-05 13:13:44,239 INFO] => creating model
[2025-02-05 13:13:48,292 INFO] CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
/home/is/tomoyuki-j/miniconda3/envs/pytorch3.13/lib/python3.13/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-02-05 13:13:50,156 INFO] module.log_inv_t: 1.0
[2025-02-05 13:13:50,157 INFO] module.hr_bert.embeddings.word_embeddings.weight: 23440896
[2025-02-05 13:13:50,157 INFO] module.hr_bert.embeddings.position_embeddings.weight: 393216
[2025-02-05 13:13:50,157 INFO] module.hr_bert.embeddings.token_type_embeddings.weight: 1536
[2025-02-05 13:13:50,157 INFO] module.hr_bert.embeddings.LayerNorm.weight: 768
[2025-02-05 13:13:50,157 INFO] module.hr_bert.embeddings.LayerNorm.bias: 768
[2025-02-05 13:13:50,157 INFO] module.hr_bert.encoder.layer.0.attention.self.query.weight: 589824
[2025-02-05 13:13:50,157 INFO] module.hr_bert.encoder.layer.0.attention.self.query.bias: 768
[2025-02-05 13:13:50,157 INFO] module.hr_bert.encoder.layer.0.attention.self.key.weight: 589824
[2025-02-05 13:13:50,157 INFO] module.hr_bert.encoder.layer.0.attention.self.key.bias: 768
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.attention.self.value.weight: 589824
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.attention.self.value.bias: 768
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.attention.output.dense.bias: 768
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.output.dense.weight: 2359296
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.output.dense.bias: 768
[2025-02-05 13:13:50,158 INFO] module.hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.1.attention.self.query.weight: 589824
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.1.attention.self.query.bias: 768
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.1.attention.self.key.weight: 589824
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.1.attention.self.key.bias: 768
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.1.attention.self.value.weight: 589824
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.1.attention.self.value.bias: 768
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.1.attention.output.dense.bias: 768
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,159 INFO] module.hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.1.output.dense.weight: 2359296
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.1.output.dense.bias: 768
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.2.attention.self.query.weight: 589824
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.2.attention.self.query.bias: 768
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.2.attention.self.key.weight: 589824
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.2.attention.self.key.bias: 768
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.2.attention.self.value.weight: 589824
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.2.attention.self.value.bias: 768
[2025-02-05 13:13:50,160 INFO] module.hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.2.attention.output.dense.bias: 768
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.2.output.dense.weight: 2359296
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.2.output.dense.bias: 768
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.3.attention.self.query.weight: 589824
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.3.attention.self.query.bias: 768
[2025-02-05 13:13:50,161 INFO] module.hr_bert.encoder.layer.3.attention.self.key.weight: 589824
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.attention.self.key.bias: 768
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.attention.self.value.weight: 589824
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.attention.self.value.bias: 768
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.attention.output.dense.bias: 768
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.output.dense.weight: 2359296
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.output.dense.bias: 768
[2025-02-05 13:13:50,162 INFO] module.hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.attention.self.query.weight: 589824
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.attention.self.query.bias: 768
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.attention.self.key.weight: 589824
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.attention.self.key.bias: 768
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.attention.self.value.weight: 589824
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.attention.self.value.bias: 768
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.attention.output.dense.bias: 768
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,163 INFO] module.hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.4.output.dense.weight: 2359296
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.4.output.dense.bias: 768
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.5.attention.self.query.weight: 589824
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.5.attention.self.query.bias: 768
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.5.attention.self.key.weight: 589824
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.5.attention.self.key.bias: 768
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.5.attention.self.value.weight: 589824
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.5.attention.self.value.bias: 768
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,164 INFO] module.hr_bert.encoder.layer.5.attention.output.dense.bias: 768
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.5.output.dense.weight: 2359296
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.5.output.dense.bias: 768
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.6.attention.self.query.weight: 589824
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.6.attention.self.query.bias: 768
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.6.attention.self.key.weight: 589824
[2025-02-05 13:13:50,165 INFO] module.hr_bert.encoder.layer.6.attention.self.key.bias: 768
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.attention.self.value.weight: 589824
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.attention.self.value.bias: 768
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.attention.output.dense.bias: 768
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.output.dense.weight: 2359296
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.output.dense.bias: 768
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,166 INFO] module.hr_bert.encoder.layer.7.attention.self.query.weight: 589824
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.attention.self.query.bias: 768
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.attention.self.key.weight: 589824
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.attention.self.key.bias: 768
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.attention.self.value.weight: 589824
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.attention.self.value.bias: 768
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.attention.output.dense.bias: 768
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,167 INFO] module.hr_bert.encoder.layer.7.output.dense.weight: 2359296
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.7.output.dense.bias: 768
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.8.attention.self.query.weight: 589824
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.8.attention.self.query.bias: 768
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.8.attention.self.key.weight: 589824
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.8.attention.self.key.bias: 768
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.8.attention.self.value.weight: 589824
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.8.attention.self.value.bias: 768
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.8.attention.output.dense.bias: 768
[2025-02-05 13:13:50,168 INFO] module.hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.8.output.dense.weight: 2359296
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.8.output.dense.bias: 768
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.9.attention.self.query.weight: 589824
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.9.attention.self.query.bias: 768
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.9.attention.self.key.weight: 589824
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.9.attention.self.key.bias: 768
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.9.attention.self.value.weight: 589824
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.9.attention.self.value.bias: 768
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.9.attention.output.dense.bias: 768
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,169 INFO] module.hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.9.output.dense.weight: 2359296
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.9.output.dense.bias: 768
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.attention.self.query.weight: 589824
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.attention.self.query.bias: 768
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.attention.self.key.weight: 589824
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.attention.self.key.bias: 768
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.attention.self.value.weight: 589824
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.attention.self.value.bias: 768
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.attention.output.dense.bias: 768
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,170 INFO] module.hr_bert.encoder.layer.10.output.dense.weight: 2359296
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.10.output.dense.bias: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.attention.self.query.weight: 589824
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.attention.self.query.bias: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.attention.self.key.weight: 589824
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.attention.self.key.bias: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.attention.self.value.weight: 589824
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.attention.self.value.bias: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.attention.output.dense.bias: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.output.dense.weight: 2359296
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.output.dense.bias: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,171 INFO] module.hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,172 INFO] module.hr_bert.pooler.dense.weight: 589824
[2025-02-05 13:13:50,172 INFO] module.hr_bert.pooler.dense.bias: 768
[2025-02-05 13:13:50,172 INFO] module.tail_bert.embeddings.word_embeddings.weight: 23440896
[2025-02-05 13:13:50,172 INFO] module.tail_bert.embeddings.position_embeddings.weight: 393216
[2025-02-05 13:13:50,172 INFO] module.tail_bert.embeddings.token_type_embeddings.weight: 1536
[2025-02-05 13:13:50,172 INFO] module.tail_bert.embeddings.LayerNorm.weight: 768
[2025-02-05 13:13:50,172 INFO] module.tail_bert.embeddings.LayerNorm.bias: 768
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.attention.self.query.weight: 589824
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.attention.self.query.bias: 768
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.attention.self.key.weight: 589824
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.attention.self.key.bias: 768
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.attention.self.value.weight: 589824
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.attention.self.value.bias: 768
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.attention.output.dense.bias: 768
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,172 INFO] module.tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.0.output.dense.weight: 2359296
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.0.output.dense.bias: 768
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.attention.self.query.weight: 589824
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.attention.self.query.bias: 768
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.attention.self.key.weight: 589824
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.attention.self.key.bias: 768
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.attention.self.value.weight: 589824
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.attention.self.value.bias: 768
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.attention.output.dense.bias: 768
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.output.dense.weight: 2359296
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.output.dense.bias: 768
[2025-02-05 13:13:50,173 INFO] module.tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.attention.self.query.weight: 589824
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.attention.self.query.bias: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.attention.self.key.weight: 589824
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.attention.self.key.bias: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.attention.self.value.weight: 589824
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.attention.self.value.bias: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.attention.output.dense.bias: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.output.dense.weight: 2359296
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.output.dense.bias: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.3.attention.self.query.weight: 589824
[2025-02-05 13:13:50,174 INFO] module.tail_bert.encoder.layer.3.attention.self.query.bias: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.attention.self.key.weight: 589824
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.attention.self.key.bias: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.attention.self.value.weight: 589824
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.attention.self.value.bias: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.attention.output.dense.bias: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.output.dense.weight: 2359296
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.output.dense.bias: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.4.attention.self.query.weight: 589824
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.4.attention.self.query.bias: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.4.attention.self.key.weight: 589824
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.4.attention.self.key.bias: 768
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.4.attention.self.value.weight: 589824
[2025-02-05 13:13:50,175 INFO] module.tail_bert.encoder.layer.4.attention.self.value.bias: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.4.attention.output.dense.bias: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.4.output.dense.weight: 2359296
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.4.output.dense.bias: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.5.attention.self.query.weight: 589824
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.5.attention.self.query.bias: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.5.attention.self.key.weight: 589824
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.5.attention.self.key.bias: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.5.attention.self.value.weight: 589824
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.5.attention.self.value.bias: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.5.attention.output.dense.bias: 768
[2025-02-05 13:13:50,176 INFO] module.tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.5.output.dense.weight: 2359296
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.5.output.dense.bias: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.attention.self.query.weight: 589824
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.attention.self.query.bias: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.attention.self.key.weight: 589824
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.attention.self.key.bias: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.attention.self.value.weight: 589824
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.attention.self.value.bias: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.attention.output.dense.bias: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,177 INFO] module.tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.6.output.dense.weight: 2359296
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.6.output.dense.bias: 768
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.attention.self.query.weight: 589824
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.attention.self.query.bias: 768
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.attention.self.key.weight: 589824
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.attention.self.key.bias: 768
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.attention.self.value.weight: 589824
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.attention.self.value.bias: 768
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.attention.output.dense.bias: 768
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.output.dense.weight: 2359296
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.output.dense.bias: 768
[2025-02-05 13:13:50,178 INFO] module.tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.attention.self.query.weight: 589824
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.attention.self.query.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.attention.self.key.weight: 589824
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.attention.self.key.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.attention.self.value.weight: 589824
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.attention.self.value.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.attention.output.dense.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.output.dense.weight: 2359296
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.output.dense.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.9.attention.self.query.weight: 589824
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.9.attention.self.query.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.9.attention.self.key.weight: 589824
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.9.attention.self.key.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.9.attention.self.value.weight: 589824
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.9.attention.self.value.bias: 768
[2025-02-05 13:13:50,179 INFO] module.tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.9.attention.output.dense.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.9.output.dense.weight: 2359296
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.9.output.dense.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.attention.self.query.weight: 589824
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.attention.self.query.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.attention.self.key.weight: 589824
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.attention.self.key.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.attention.self.value.weight: 589824
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.attention.self.value.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.attention.output.dense.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.output.dense.weight: 2359296
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.output.dense.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.11.attention.self.query.weight: 589824
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.11.attention.self.query.bias: 768
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.11.attention.self.key.weight: 589824
[2025-02-05 13:13:50,180 INFO] module.tail_bert.encoder.layer.11.attention.self.key.bias: 768
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.attention.self.value.weight: 589824
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.attention.self.value.bias: 768
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.attention.output.dense.bias: 768
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.output.dense.weight: 2359296
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.output.dense.bias: 768
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
[2025-02-05 13:13:50,181 INFO] module.tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
[2025-02-05 13:13:50,181 INFO] module.tail_bert.pooler.dense.weight: 589824
[2025-02-05 13:13:50,181 INFO] module.tail_bert.pooler.dense.bias: 768
[2025-02-05 13:13:50,181 INFO] Number of parameters: 218.0M
[2025-02-05 13:13:50,181 INFO] In test mode: False
[2025-02-05 13:13:50,245 INFO] Load 57120 examples from /cl/work8/tomoyuki-j/Projects/simkgc/SimKGC/data/WN18RR/train.txt.json
[2025-02-05 13:13:50,513 INFO] In test mode: False
[2025-02-05 13:13:50,530 INFO] Load 14858 examples from /cl/work8/tomoyuki-j/Projects/simkgc/SimKGC/data/WN18RR/valid.txt.json
[2025-02-05 13:13:50,555 INFO] Total training steps: 5578, warmup steps: 400
[2025-02-05 13:13:50,555 INFO] Args={
    "pretrained_model": "bert-base-uncased",
    "task": "WN18RR",
    "train_path": "/cl/work8/tomoyuki-j/Projects/simkgc/SimKGC/data/WN18RR/train.txt.json",
    "valid_path": "/cl/work8/tomoyuki-j/Projects/simkgc/SimKGC/data/WN18RR/valid.txt.json",
    "model_dir": "./checkpoint/wn18rr/",
    "warmup": 400,
    "max_to_keep": 3,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": true,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 4,
    "epochs": 50,
    "batch_size": 1024,
    "lr": 5e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": null,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
/project/nlp-work8/tomoyuki-j/Projects/simkgc/SimKGC/trainer.py:72: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
/project/nlp-work8/tomoyuki-j/Projects/simkgc/SimKGC/trainer.py:150: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
[2025-02-05 13:13:55,814 INFO] Epoch: [0][  0/111]	Loss 10.69 (10.69)	InvT  20.00 ( 20.00)	Acc@1   0.00 (  0.00)	Acc@3  16.80 ( 16.80)
[2025-02-05 13:14:09,816 INFO] Epoch: [0][ 20/111]	Loss 10.07 (10.31)	InvT  20.00 ( 20.00)	Acc@1   0.00 (  0.00)	Acc@3  22.27 ( 18.96)
[2025-02-05 13:14:23,805 INFO] Epoch: [0][ 40/111]	Loss 8.961 (9.867)	InvT  20.00 ( 20.00)	Acc@1   0.00 (  0.00)	Acc@3  26.37 ( 22.08)
[2025-02-05 13:14:37,988 INFO] Epoch: [0][ 60/111]	Loss 8.097 (9.434)	InvT  20.00 ( 20.00)	Acc@1   0.00 (  0.00)	Acc@3  32.71 ( 24.52)
[2025-02-05 13:14:51,937 INFO] Epoch: [0][ 80/111]	Loss 7.318 (9.007)	InvT  19.99 ( 20.00)	Acc@1   0.49 (  0.03)	Acc@3  37.11 ( 27.25)
[2025-02-05 13:15:05,932 INFO] Epoch: [0][100/111]	Loss 6.191 (8.549)	InvT  19.99 ( 20.00)	Acc@1   8.50 (  1.16)	Acc@3  49.41 ( 30.67)
[2025-02-05 13:15:12,708 INFO] Learning rate: 1.3875000000000002e-05
[2025-02-05 13:15:28,121 INFO] Epoch 0, valid metric: {"Acc@1": 6.609, "Acc@3": 14.851, "loss": 5.838}
[2025-02-05 13:15:39,242 INFO] Epoch: [1][  0/111]	Loss 5.781 (5.781)	InvT  19.99 ( 19.99)	Acc@1  16.02 ( 16.02)	Acc@3  53.42 ( 53.42)
[2025-02-05 13:15:53,573 INFO] Epoch: [1][ 20/111]	Loss 5.02 (5.29)	InvT  19.99 ( 19.99)	Acc@1  25.20 ( 21.17)	Acc@3  61.62 ( 57.45)
[2025-02-05 13:16:07,660 INFO] Epoch: [1][ 40/111]	Loss 4.63 (5.025)	InvT  19.99 ( 19.99)	Acc@1  29.00 ( 24.48)	Acc@3  64.06 ( 60.00)
[2025-02-05 13:16:21,447 INFO] Epoch: [1][ 60/111]	Loss 4.232 (4.797)	InvT  19.99 ( 19.99)	Acc@1  35.35 ( 27.32)	Acc@3  69.34 ( 62.19)
[2025-02-05 13:16:35,763 INFO] Epoch: [1][ 80/111]	Loss 3.912 (4.625)	InvT  19.99 ( 19.99)	Acc@1  40.23 ( 29.49)	Acc@3  69.92 ( 63.89)
[2025-02-05 13:16:49,558 INFO] Epoch: [1][100/111]	Loss 3.918 (4.47)	InvT  19.98 ( 19.99)	Acc@1  37.89 ( 31.67)	Acc@3  70.21 ( 65.39)
[2025-02-05 13:16:56,517 INFO] Learning rate: 2.7750000000000004e-05
[2025-02-05 13:17:11,999 INFO] Epoch 1, valid metric: {"Acc@1": 5.721, "Acc@3": 12.895, "loss": 6.558}
[2025-02-05 13:17:20,044 INFO] Epoch: [2][  0/111]	Loss 3.438 (3.438)	InvT  19.98 ( 19.98)	Acc@1  45.12 ( 45.12)	Acc@3  73.83 ( 73.83)
[2025-02-05 13:17:34,179 INFO] Epoch: [2][ 20/111]	Loss 3.208 (3.333)	InvT  19.99 ( 19.98)	Acc@1  49.32 ( 47.47)	Acc@3  77.73 ( 76.52)
[2025-02-05 13:17:48,045 INFO] Epoch: [2][ 40/111]	Loss 3.224 (3.246)	InvT  19.99 ( 19.99)	Acc@1  47.66 ( 48.46)	Acc@3  78.32 ( 77.31)
[2025-02-05 13:18:02,079 INFO] Epoch: [2][ 60/111]	Loss 3.068 (3.192)	InvT  19.99 ( 19.99)	Acc@1  51.56 ( 49.18)	Acc@3  79.59 ( 77.88)
[2025-02-05 13:18:16,021 INFO] Epoch: [2][ 80/111]	Loss 2.964 (3.135)	InvT  19.99 ( 19.99)	Acc@1  54.49 ( 50.01)	Acc@3  80.96 ( 78.47)
[2025-02-05 13:18:29,972 INFO] Epoch: [2][100/111]	Loss 2.736 (3.077)	InvT  19.99 ( 19.99)	Acc@1  55.08 ( 50.91)	Acc@3  81.25 ( 79.03)
[2025-02-05 13:18:36,810 INFO] Learning rate: 4.1625e-05
[2025-02-05 13:18:52,181 INFO] Epoch 2, valid metric: {"Acc@1": 6.488, "Acc@3": 14.773, "loss": 6.424}
[2025-02-05 13:18:59,931 INFO] Epoch: [3][  0/111]	Loss 2.332 (2.332)	InvT  19.99 ( 19.99)	Acc@1  61.13 ( 61.13)	Acc@3  85.35 ( 85.35)
[2025-02-05 13:19:14,207 INFO] Epoch: [3][ 20/111]	Loss 2.32 (2.406)	InvT  20.00 ( 20.00)	Acc@1  59.96 ( 60.00)	Acc@3  85.06 ( 85.44)
[2025-02-05 13:19:28,124 INFO] Epoch: [3][ 40/111]	Loss 2.277 (2.339)	InvT  20.01 ( 20.00)	Acc@1  61.13 ( 60.94)	Acc@3  86.52 ( 85.87)
[2025-02-05 13:19:41,952 INFO] Epoch: [3][ 60/111]	Loss 2.447 (2.315)	InvT  20.02 ( 20.01)	Acc@1  59.47 ( 61.47)	Acc@3  84.96 ( 86.12)
[2025-02-05 13:19:56,099 INFO] Epoch: [3][ 80/111]	Loss 2.184 (2.297)	InvT  20.04 ( 20.01)	Acc@1  62.79 ( 61.83)	Acc@3  87.11 ( 86.23)
[2025-02-05 13:20:10,133 INFO] Epoch: [3][100/111]	Loss 2.242 (2.275)	InvT  20.05 ( 20.02)	Acc@1  63.57 ( 62.31)	Acc@3  86.13 ( 86.38)
[2025-02-05 13:20:16,923 INFO] Learning rate: 4.957512553109309e-05
[2025-02-05 13:20:32,326 INFO] Epoch 3, valid metric: {"Acc@1": 6.623, "Acc@3": 15.09, "loss": 6.301}
[2025-02-05 13:20:41,030 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch0.mdl
[2025-02-05 13:20:43,318 INFO] Epoch: [4][  0/111]	Loss 1.838 (1.838)	InvT  20.05 ( 20.05)	Acc@1  69.53 ( 69.53)	Acc@3  89.84 ( 89.84)
[2025-02-05 13:20:57,820 INFO] Epoch: [4][ 20/111]	Loss 1.819 (1.729)	InvT  20.07 ( 20.06)	Acc@1  68.55 ( 70.16)	Acc@3  90.72 ( 91.34)
[2025-02-05 13:21:11,857 INFO] Epoch: [4][ 40/111]	Loss 1.671 (1.716)	InvT  20.09 ( 20.07)	Acc@1  71.29 ( 70.33)	Acc@3  91.89 ( 91.32)
[2025-02-05 13:21:25,693 INFO] Epoch: [4][ 60/111]	Loss 1.675 (1.713)	InvT  20.11 ( 20.08)	Acc@1  68.85 ( 70.38)	Acc@3  92.48 ( 91.33)
[2025-02-05 13:21:39,644 INFO] Epoch: [4][ 80/111]	Loss 1.556 (1.706)	InvT  20.13 ( 20.09)	Acc@1  72.85 ( 70.49)	Acc@3  92.48 ( 91.32)
[2025-02-05 13:21:53,655 INFO] Epoch: [4][100/111]	Loss 1.707 (1.707)	InvT  20.14 ( 20.10)	Acc@1  69.63 ( 70.50)	Acc@3  91.31 ( 91.31)
[2025-02-05 13:22:00,456 INFO] Learning rate: 4.8503283120896105e-05
[2025-02-05 13:22:16,031 INFO] Epoch 4, valid metric: {"Acc@1": 6.613, "Acc@3": 14.901, "loss": 6.337}
[2025-02-05 13:22:21,669 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch1.mdl
[2025-02-05 13:22:23,801 INFO] Epoch: [5][  0/111]	Loss 1.351 (1.351)	InvT  20.15 ( 20.15)	Acc@1  76.37 ( 76.37)	Acc@3  94.63 ( 94.63)
[2025-02-05 13:22:37,836 INFO] Epoch: [5][ 20/111]	Loss 1.504 (1.319)	InvT  20.17 ( 20.16)	Acc@1  74.02 ( 76.52)	Acc@3  93.26 ( 94.40)
[2025-02-05 13:22:51,996 INFO] Epoch: [5][ 40/111]	Loss 1.375 (1.305)	InvT  20.20 ( 20.17)	Acc@1  74.41 ( 76.77)	Acc@3  92.77 ( 94.46)
[2025-02-05 13:23:05,783 INFO] Epoch: [5][ 60/111]	Loss 1.179 (1.301)	InvT  20.22 ( 20.18)	Acc@1  79.98 ( 76.91)	Acc@3  95.41 ( 94.49)
[2025-02-05 13:23:19,810 INFO] Epoch: [5][ 80/111]	Loss 1.316 (1.305)	InvT  20.24 ( 20.19)	Acc@1  76.95 ( 76.84)	Acc@3  94.14 ( 94.43)
[2025-02-05 13:23:33,953 INFO] Epoch: [5][100/111]	Loss 1.349 (1.306)	InvT  20.26 ( 20.21)	Acc@1  76.56 ( 76.94)	Acc@3  94.14 ( 94.41)
[2025-02-05 13:23:40,777 INFO] Learning rate: 4.7431440710699116e-05
[2025-02-05 13:23:56,363 INFO] Epoch 5, valid metric: {"Acc@1": 7.39, "Acc@3": 16.762, "loss": 6.246}
[2025-02-05 13:24:05,315 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch2.mdl
[2025-02-05 13:24:07,508 INFO] Epoch: [6][  0/111]	Loss 0.9731 (0.9731)	InvT  20.27 ( 20.27)	Acc@1  82.52 ( 82.52)	Acc@3  97.07 ( 97.07)
[2025-02-05 13:24:21,446 INFO] Epoch: [6][ 20/111]	Loss 1.078 (1.016)	InvT  20.29 ( 20.28)	Acc@1  82.42 ( 81.79)	Acc@3  96.09 ( 96.74)
[2025-02-05 13:24:35,498 INFO] Epoch: [6][ 40/111]	Loss 0.8572 (1.014)	InvT  20.32 ( 20.29)	Acc@1  82.91 ( 81.77)	Acc@3  97.17 ( 96.55)
[2025-02-05 13:24:49,500 INFO] Epoch: [6][ 60/111]	Loss 0.8986 (1.016)	InvT  20.34 ( 20.30)	Acc@1  82.62 ( 81.85)	Acc@3  97.46 ( 96.53)
[2025-02-05 13:25:03,399 INFO] Epoch: [6][ 80/111]	Loss 1.062 (1.023)	InvT  20.36 ( 20.32)	Acc@1  82.91 ( 81.80)	Acc@3  96.09 ( 96.42)
[2025-02-05 13:25:17,508 INFO] Epoch: [6][100/111]	Loss 1.178 (1.026)	InvT  20.38 ( 20.33)	Acc@1  78.12 ( 81.76)	Acc@3  95.41 ( 96.38)
[2025-02-05 13:25:24,317 INFO] Learning rate: 4.6359598300502126e-05
[2025-02-05 13:25:40,057 INFO] Epoch 6, valid metric: {"Acc@1": 6.411, "Acc@3": 14.423, "loss": 6.609}
[2025-02-05 13:25:46,010 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch3.mdl
[2025-02-05 13:25:48,202 INFO] Epoch: [7][  0/111]	Loss 0.7296 (0.7296)	InvT  20.39 ( 20.39)	Acc@1  88.28 ( 88.28)	Acc@3  98.05 ( 98.05)
[2025-02-05 13:26:02,394 INFO] Epoch: [7][ 20/111]	Loss 0.8092 (0.8039)	InvT  20.42 ( 20.40)	Acc@1  84.57 ( 85.61)	Acc@3  97.85 ( 97.89)
[2025-02-05 13:26:16,413 INFO] Epoch: [7][ 40/111]	Loss 0.7759 (0.8073)	InvT  20.44 ( 20.42)	Acc@1  85.35 ( 85.58)	Acc@3  97.66 ( 97.78)
[2025-02-05 13:26:30,378 INFO] Epoch: [7][ 60/111]	Loss 0.9262 (0.8181)	InvT  20.46 ( 20.43)	Acc@1  83.89 ( 85.39)	Acc@3  96.68 ( 97.71)
[2025-02-05 13:26:44,146 INFO] Epoch: [7][ 80/111]	Loss 0.878 (0.8211)	InvT  20.48 ( 20.44)	Acc@1  83.20 ( 85.36)	Acc@3  97.17 ( 97.67)
[2025-02-05 13:26:58,120 INFO] Epoch: [7][100/111]	Loss 0.8841 (0.8311)	InvT  20.50 ( 20.45)	Acc@1  84.86 ( 85.18)	Acc@3  97.75 ( 97.61)
[2025-02-05 13:27:04,925 INFO] Learning rate: 4.528775589030514e-05
[2025-02-05 13:27:20,494 INFO] Epoch 7, valid metric: {"Acc@1": 7.373, "Acc@3": 16.967, "loss": 6.344}
[2025-02-05 13:27:26,744 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch4.mdl
[2025-02-05 13:27:29,016 INFO] Epoch: [8][  0/111]	Loss 0.7288 (0.7288)	InvT  20.51 ( 20.51)	Acc@1  86.04 ( 86.04)	Acc@3  98.14 ( 98.14)
[2025-02-05 13:27:43,046 INFO] Epoch: [8][ 20/111]	Loss 0.6878 (0.6556)	InvT  20.53 ( 20.52)	Acc@1  88.09 ( 88.48)	Acc@3  98.83 ( 98.54)
[2025-02-05 13:27:56,892 INFO] Epoch: [8][ 40/111]	Loss 0.7749 (0.6569)	InvT  20.56 ( 20.53)	Acc@1  87.11 ( 88.43)	Acc@3  97.66 ( 98.56)
[2025-02-05 13:28:10,814 INFO] Epoch: [8][ 60/111]	Loss 0.6684 (0.6615)	InvT  20.58 ( 20.55)	Acc@1  87.89 ( 88.34)	Acc@3  98.54 ( 98.54)
[2025-02-05 13:28:24,649 INFO] Epoch: [8][ 80/111]	Loss 0.6135 (0.6703)	InvT  20.60 ( 20.56)	Acc@1  89.26 ( 88.17)	Acc@3  98.24 ( 98.45)
[2025-02-05 13:28:38,737 INFO] Epoch: [8][100/111]	Loss 0.6449 (0.6748)	InvT  20.62 ( 20.57)	Acc@1  88.38 ( 88.08)	Acc@3  98.54 ( 98.42)
[2025-02-05 13:28:45,551 INFO] Learning rate: 4.421591348010815e-05
[2025-02-05 13:29:01,335 INFO] Epoch 8, valid metric: {"Acc@1": 7.279, "Acc@3": 16.755, "loss": 6.318}
[2025-02-05 13:29:07,669 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch5.mdl
[2025-02-05 13:29:09,868 INFO] Epoch: [9][  0/111]	Loss 0.553 (0.553)	InvT  20.63 ( 20.63)	Acc@1  90.82 ( 90.82)	Acc@3  99.51 ( 99.51)
[2025-02-05 13:29:23,935 INFO] Epoch: [9][ 20/111]	Loss 0.4848 (0.5316)	InvT  20.65 ( 20.64)	Acc@1  91.60 ( 90.89)	Acc@3  99.22 ( 99.21)
[2025-02-05 13:29:38,024 INFO] Epoch: [9][ 40/111]	Loss 0.5862 (0.546)	InvT  20.67 ( 20.65)	Acc@1  89.36 ( 90.61)	Acc@3  99.12 ( 99.09)
[2025-02-05 13:29:51,960 INFO] Epoch: [9][ 60/111]	Loss 0.5714 (0.5495)	InvT  20.69 ( 20.66)	Acc@1  90.82 ( 90.61)	Acc@3  99.12 ( 99.07)
[2025-02-05 13:30:05,959 INFO] Epoch: [9][ 80/111]	Loss 0.5177 (0.5531)	InvT  20.71 ( 20.67)	Acc@1  90.62 ( 90.49)	Acc@3  99.61 ( 99.01)
[2025-02-05 13:30:19,820 INFO] Epoch: [9][100/111]	Loss 0.6197 (0.5612)	InvT  20.72 ( 20.68)	Acc@1  89.45 ( 90.29)	Acc@3  98.83 ( 98.97)
[2025-02-05 13:30:26,630 INFO] Learning rate: 4.3144071069911165e-05
[2025-02-05 13:30:42,461 INFO] Epoch 9, valid metric: {"Acc@1": 7.454, "Acc@3": 16.934, "loss": 6.267}
[2025-02-05 13:30:50,856 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch6.mdl
[2025-02-05 13:30:53,036 INFO] Epoch: [10][  0/111]	Loss 0.5126 (0.5126)	InvT  20.73 ( 20.73)	Acc@1  91.31 ( 91.31)	Acc@3  99.32 ( 99.32)
[2025-02-05 13:31:07,117 INFO] Epoch: [10][ 20/111]	Loss 0.4461 (0.4594)	InvT  20.75 ( 20.74)	Acc@1  92.97 ( 92.49)	Acc@3  99.12 ( 99.45)
[2025-02-05 13:31:21,077 INFO] Epoch: [10][ 40/111]	Loss 0.4985 (0.4694)	InvT  20.77 ( 20.75)	Acc@1  90.62 ( 92.22)	Acc@3  99.61 ( 99.40)
[2025-02-05 13:31:34,949 INFO] Epoch: [10][ 60/111]	Loss 0.4418 (0.4731)	InvT  20.79 ( 20.76)	Acc@1  93.26 ( 92.10)	Acc@3  99.41 ( 99.39)
[2025-02-05 13:31:48,755 INFO] Epoch: [10][ 80/111]	Loss 0.5414 (0.4769)	InvT  20.81 ( 20.77)	Acc@1  90.53 ( 91.98)	Acc@3  99.22 ( 99.35)
[2025-02-05 13:32:02,705 INFO] Epoch: [10][100/111]	Loss 0.4832 (0.4818)	InvT  20.83 ( 20.78)	Acc@1  91.60 ( 91.88)	Acc@3  99.71 ( 99.31)
[2025-02-05 13:32:09,494 INFO] Learning rate: 4.2072228659714176e-05
[2025-02-05 13:32:25,080 INFO] Epoch 10, valid metric: {"Acc@1": 7.706, "Acc@3": 17.842, "loss": 6.349}
[2025-02-05 13:32:34,488 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch7.mdl
[2025-02-05 13:32:36,635 INFO] Epoch: [11][  0/111]	Loss 0.3972 (0.3972)	InvT  20.84 ( 20.84)	Acc@1  93.16 ( 93.16)	Acc@3  99.61 ( 99.61)
[2025-02-05 13:32:50,693 INFO] Epoch: [11][ 20/111]	Loss 0.4232 (0.4038)	InvT  20.85 ( 20.84)	Acc@1  93.26 ( 93.46)	Acc@3  99.80 ( 99.57)
[2025-02-05 13:33:04,836 INFO] Epoch: [11][ 40/111]	Loss 0.4145 (0.3997)	InvT  20.87 ( 20.85)	Acc@1  91.99 ( 93.43)	Acc@3  99.32 ( 99.59)
[2025-02-05 13:33:18,603 INFO] Epoch: [11][ 60/111]	Loss 0.3785 (0.4004)	InvT  20.89 ( 20.86)	Acc@1  93.55 ( 93.45)	Acc@3  99.61 ( 99.60)
[2025-02-05 13:33:32,504 INFO] Epoch: [11][ 80/111]	Loss 0.4641 (0.4029)	InvT  20.90 ( 20.87)	Acc@1  92.19 ( 93.40)	Acc@3  99.51 ( 99.59)
[2025-02-05 13:33:46,540 INFO] Epoch: [11][100/111]	Loss 0.3949 (0.4057)	InvT  20.92 ( 20.88)	Acc@1  93.75 ( 93.32)	Acc@3  99.32 ( 99.56)
[2025-02-05 13:33:53,324 INFO] Learning rate: 4.100038624951719e-05
[2025-02-05 13:34:08,955 INFO] Epoch 11, valid metric: {"Acc@1": 7.632, "Acc@3": 17.681, "loss": 6.367}
[2025-02-05 13:34:15,356 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch8.mdl
[2025-02-05 13:34:17,585 INFO] Epoch: [12][  0/111]	Loss 0.3186 (0.3186)	InvT  20.93 ( 20.93)	Acc@1  95.21 ( 95.21)	Acc@3  99.80 ( 99.80)
[2025-02-05 13:34:31,616 INFO] Epoch: [12][ 20/111]	Loss 0.3196 (0.3455)	InvT  20.95 ( 20.94)	Acc@1  95.31 ( 94.70)	Acc@3  99.80 ( 99.73)
[2025-02-05 13:34:45,534 INFO] Epoch: [12][ 40/111]	Loss 0.3588 (0.3417)	InvT  20.96 ( 20.95)	Acc@1  94.34 ( 94.67)	Acc@3  99.90 ( 99.73)
[2025-02-05 13:34:59,534 INFO] Epoch: [12][ 60/111]	Loss 0.4035 (0.3444)	InvT  20.98 ( 20.95)	Acc@1  92.87 ( 94.62)	Acc@3  99.71 ( 99.72)
[2025-02-05 13:35:13,381 INFO] Epoch: [12][ 80/111]	Loss 0.335 (0.3447)	InvT  20.99 ( 20.96)	Acc@1  95.70 ( 94.65)	Acc@3  99.71 ( 99.71)
[2025-02-05 13:35:27,370 INFO] Epoch: [12][100/111]	Loss 0.3131 (0.3491)	InvT  21.01 ( 20.97)	Acc@1  95.41 ( 94.54)	Acc@3  99.80 ( 99.70)
[2025-02-05 13:35:34,170 INFO] Learning rate: 3.9928543839320204e-05
[2025-02-05 13:35:49,764 INFO] Epoch 12, valid metric: {"Acc@1": 7.575, "Acc@3": 16.9, "loss": 6.417}
[2025-02-05 13:35:56,498 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch9.mdl
[2025-02-05 13:35:58,711 INFO] Epoch: [13][  0/111]	Loss 0.2754 (0.2754)	InvT  21.02 ( 21.02)	Acc@1  95.80 ( 95.80)	Acc@3 100.00 (100.00)
[2025-02-05 13:36:12,596 INFO] Epoch: [13][ 20/111]	Loss 0.2406 (0.2887)	InvT  21.03 ( 21.03)	Acc@1  96.29 ( 95.62)	Acc@3  99.71 ( 99.81)
[2025-02-05 13:36:26,527 INFO] Epoch: [13][ 40/111]	Loss 0.2764 (0.2962)	InvT  21.05 ( 21.03)	Acc@1  96.39 ( 95.45)	Acc@3  99.71 ( 99.80)
[2025-02-05 13:36:40,437 INFO] Epoch: [13][ 60/111]	Loss 0.3027 (0.2962)	InvT  21.06 ( 21.04)	Acc@1  95.70 ( 95.43)	Acc@3  99.80 ( 99.82)
[2025-02-05 13:36:54,231 INFO] Epoch: [13][ 80/111]	Loss 0.2654 (0.2998)	InvT  21.08 ( 21.05)	Acc@1  96.58 ( 95.35)	Acc@3 100.00 ( 99.81)
[2025-02-05 13:37:08,315 INFO] Epoch: [13][100/111]	Loss 0.303 (0.3028)	InvT  21.09 ( 21.06)	Acc@1  95.51 ( 95.29)	Acc@3  99.80 ( 99.80)
[2025-02-05 13:37:15,152 INFO] Learning rate: 3.8856701429123215e-05
[2025-02-05 13:37:31,064 INFO] Epoch 13, valid metric: {"Acc@1": 8.127, "Acc@3": 18.152, "loss": 6.31}
[2025-02-05 13:37:41,354 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch10.mdl
[2025-02-05 13:37:43,651 INFO] Epoch: [14][  0/111]	Loss 0.2198 (0.2198)	InvT  21.10 ( 21.10)	Acc@1  97.07 ( 97.07)	Acc@3 100.00 (100.00)
[2025-02-05 13:37:57,545 INFO] Epoch: [14][ 20/111]	Loss 0.2655 (0.2648)	InvT  21.11 ( 21.11)	Acc@1  96.09 ( 96.06)	Acc@3  99.90 ( 99.88)
[2025-02-05 13:38:11,493 INFO] Epoch: [14][ 40/111]	Loss 0.2919 (0.2634)	InvT  21.13 ( 21.11)	Acc@1  94.82 ( 96.14)	Acc@3  99.90 ( 99.87)
[2025-02-05 13:38:25,499 INFO] Epoch: [14][ 60/111]	Loss 0.2681 (0.2625)	InvT  21.14 ( 21.12)	Acc@1  96.48 ( 96.15)	Acc@3  99.71 ( 99.87)
[2025-02-05 13:38:39,340 INFO] Epoch: [14][ 80/111]	Loss 0.2918 (0.2642)	InvT  21.16 ( 21.13)	Acc@1  96.48 ( 96.14)	Acc@3  99.80 ( 99.87)
[2025-02-05 13:38:53,287 INFO] Epoch: [14][100/111]	Loss 0.3033 (0.2675)	InvT  21.17 ( 21.14)	Acc@1  94.53 ( 96.03)	Acc@3  99.90 ( 99.86)
[2025-02-05 13:39:00,102 INFO] Learning rate: 3.7784859018926225e-05
[2025-02-05 13:39:15,831 INFO] Epoch 14, valid metric: {"Acc@1": 7.7, "Acc@3": 17.546, "loss": 6.379}
[2025-02-05 13:39:22,817 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch11.mdl
[2025-02-05 13:39:24,933 INFO] Epoch: [15][  0/111]	Loss 0.1953 (0.1953)	InvT  21.18 ( 21.18)	Acc@1  97.36 ( 97.36)	Acc@3 100.00 (100.00)
[2025-02-05 13:39:39,023 INFO] Epoch: [15][ 20/111]	Loss 0.2136 (0.2298)	InvT  21.19 ( 21.18)	Acc@1  96.97 ( 96.74)	Acc@3 100.00 ( 99.91)
[2025-02-05 13:39:52,914 INFO] Epoch: [15][ 40/111]	Loss 0.2404 (0.2315)	InvT  21.20 ( 21.19)	Acc@1  96.00 ( 96.57)	Acc@3  99.80 ( 99.92)
[2025-02-05 13:40:06,901 INFO] Epoch: [15][ 60/111]	Loss 0.2358 (0.2352)	InvT  21.22 ( 21.20)	Acc@1  96.48 ( 96.58)	Acc@3  99.71 ( 99.92)
[2025-02-05 13:40:20,695 INFO] Epoch: [15][ 80/111]	Loss 0.238 (0.2357)	InvT  21.23 ( 21.20)	Acc@1  96.00 ( 96.59)	Acc@3 100.00 ( 99.92)
[2025-02-05 13:40:34,761 INFO] Epoch: [15][100/111]	Loss 0.2274 (0.2364)	InvT  21.24 ( 21.21)	Acc@1  96.78 ( 96.58)	Acc@3  99.90 ( 99.91)
[2025-02-05 13:40:41,580 INFO] Learning rate: 3.6713016608729236e-05
[2025-02-05 13:40:57,153 INFO] Epoch 15, valid metric: {"Acc@1": 8.08, "Acc@3": 17.849, "loss": 6.345}
[2025-02-05 13:41:02,818 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch12.mdl
[2025-02-05 13:41:05,027 INFO] Epoch: [16][  0/111]	Loss 0.2221 (0.2221)	InvT  21.25 ( 21.25)	Acc@1  96.58 ( 96.58)	Acc@3 100.00 (100.00)
[2025-02-05 13:41:19,087 INFO] Epoch: [16][ 20/111]	Loss 0.2039 (0.2039)	InvT  21.26 ( 21.26)	Acc@1  97.27 ( 97.21)	Acc@3  99.90 ( 99.96)
[2025-02-05 13:41:33,167 INFO] Epoch: [16][ 40/111]	Loss 0.1915 (0.208)	InvT  21.28 ( 21.26)	Acc@1  97.07 ( 97.06)	Acc@3  99.90 ( 99.94)
[2025-02-05 13:41:47,082 INFO] Epoch: [16][ 60/111]	Loss 0.1995 (0.2089)	InvT  21.29 ( 21.27)	Acc@1  97.07 ( 97.07)	Acc@3 100.00 ( 99.93)
[2025-02-05 13:42:00,828 INFO] Epoch: [16][ 80/111]	Loss 0.2235 (0.2127)	InvT  21.30 ( 21.28)	Acc@1  96.88 ( 97.00)	Acc@3  99.80 ( 99.93)
[2025-02-05 13:42:14,834 INFO] Epoch: [16][100/111]	Loss 0.2038 (0.2159)	InvT  21.31 ( 21.28)	Acc@1  97.95 ( 96.93)	Acc@3  99.90 ( 99.93)
[2025-02-05 13:42:21,619 INFO] Learning rate: 3.5641174198532253e-05
[2025-02-05 13:42:37,190 INFO] Epoch 16, valid metric: {"Acc@1": 7.753, "Acc@3": 17.795, "loss": 6.407}
[2025-02-05 13:42:43,357 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch13.mdl
[2025-02-05 13:42:45,571 INFO] Epoch: [17][  0/111]	Loss 0.2089 (0.2089)	InvT  21.32 ( 21.32)	Acc@1  97.46 ( 97.46)	Acc@3  99.90 ( 99.90)
[2025-02-05 13:42:59,604 INFO] Epoch: [17][ 20/111]	Loss 0.1875 (0.1922)	InvT  21.33 ( 21.33)	Acc@1  97.07 ( 97.54)	Acc@3 100.00 ( 99.96)
[2025-02-05 13:43:13,482 INFO] Epoch: [17][ 40/111]	Loss 0.1817 (0.1945)	InvT  21.34 ( 21.33)	Acc@1  97.56 ( 97.42)	Acc@3  99.90 ( 99.94)
[2025-02-05 13:43:27,494 INFO] Epoch: [17][ 60/111]	Loss 0.2067 (0.1959)	InvT  21.35 ( 21.34)	Acc@1  97.27 ( 97.38)	Acc@3  99.90 ( 99.94)
[2025-02-05 13:43:41,281 INFO] Epoch: [17][ 80/111]	Loss 0.1797 (0.197)	InvT  21.37 ( 21.34)	Acc@1  97.66 ( 97.37)	Acc@3 100.00 ( 99.94)
[2025-02-05 13:43:55,174 INFO] Epoch: [17][100/111]	Loss 0.1999 (0.1988)	InvT  21.38 ( 21.35)	Acc@1  97.17 ( 97.32)	Acc@3  99.90 ( 99.94)
[2025-02-05 13:44:01,989 INFO] Learning rate: 3.456933178833527e-05
[2025-02-05 13:44:17,602 INFO] Epoch 17, valid metric: {"Acc@1": 7.595, "Acc@3": 17.051, "loss": 6.481}
[2025-02-05 13:44:23,947 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch14.mdl
[2025-02-05 13:44:26,175 INFO] Epoch: [18][  0/111]	Loss 0.1418 (0.1418)	InvT  21.38 ( 21.38)	Acc@1  98.34 ( 98.34)	Acc@3 100.00 (100.00)
[2025-02-05 13:44:40,124 INFO] Epoch: [18][ 20/111]	Loss 0.1472 (0.1701)	InvT  21.40 ( 21.39)	Acc@1  98.54 ( 97.78)	Acc@3 100.00 ( 99.98)
[2025-02-05 13:44:54,160 INFO] Epoch: [18][ 40/111]	Loss 0.1638 (0.1757)	InvT  21.41 ( 21.40)	Acc@1  97.66 ( 97.74)	Acc@3 100.00 ( 99.96)
[2025-02-05 13:45:08,258 INFO] Epoch: [18][ 60/111]	Loss 0.2163 (0.1775)	InvT  21.42 ( 21.40)	Acc@1  97.46 ( 97.70)	Acc@3  99.80 ( 99.96)
[2025-02-05 13:45:22,215 INFO] Epoch: [18][ 80/111]	Loss 0.211 (0.1793)	InvT  21.43 ( 21.41)	Acc@1  97.17 ( 97.65)	Acc@3  99.80 ( 99.95)
[2025-02-05 13:45:36,223 INFO] Epoch: [18][100/111]	Loss 0.1822 (0.1811)	InvT  21.44 ( 21.41)	Acc@1  97.27 ( 97.59)	Acc@3 100.00 ( 99.95)
[2025-02-05 13:45:43,008 INFO] Learning rate: 3.349748937813828e-05
[2025-02-05 13:45:58,606 INFO] Epoch 18, valid metric: {"Acc@1": 7.568, "Acc@3": 17.216, "loss": 6.48}
[2025-02-05 13:46:05,086 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch15.mdl
[2025-02-05 13:46:07,332 INFO] Epoch: [19][  0/111]	Loss 0.1608 (0.1608)	InvT  21.45 ( 21.45)	Acc@1  98.05 ( 98.05)	Acc@3 100.00 (100.00)
[2025-02-05 13:46:21,341 INFO] Epoch: [19][ 20/111]	Loss 0.1589 (0.1636)	InvT  21.46 ( 21.45)	Acc@1  98.05 ( 97.83)	Acc@3 100.00 ( 99.96)
[2025-02-05 13:46:35,392 INFO] Epoch: [19][ 40/111]	Loss 0.1513 (0.1602)	InvT  21.47 ( 21.46)	Acc@1  98.05 ( 97.97)	Acc@3 100.00 ( 99.97)
[2025-02-05 13:46:49,143 INFO] Epoch: [19][ 60/111]	Loss 0.1866 (0.1614)	InvT  21.48 ( 21.46)	Acc@1  97.17 ( 97.93)	Acc@3 100.00 ( 99.97)
[2025-02-05 13:47:03,028 INFO] Epoch: [19][ 80/111]	Loss 0.189 (0.1647)	InvT  21.49 ( 21.47)	Acc@1  97.56 ( 97.87)	Acc@3 100.00 ( 99.97)
[2025-02-05 13:47:16,927 INFO] Epoch: [19][100/111]	Loss 0.1635 (0.1671)	InvT  21.50 ( 21.47)	Acc@1  97.56 ( 97.81)	Acc@3 100.00 ( 99.97)
[2025-02-05 13:47:23,767 INFO] Learning rate: 3.242564696794129e-05
[2025-02-05 13:47:39,503 INFO] Epoch 19, valid metric: {"Acc@1": 7.804, "Acc@3": 17.563, "loss": 6.457}
[2025-02-05 13:47:45,108 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch16.mdl
[2025-02-05 13:47:47,363 INFO] Epoch: [20][  0/111]	Loss 0.1765 (0.1765)	InvT  21.50 ( 21.50)	Acc@1  97.75 ( 97.75)	Acc@3 100.00 (100.00)
[2025-02-05 13:48:01,333 INFO] Epoch: [20][ 20/111]	Loss 0.1365 (0.1497)	InvT  21.51 ( 21.51)	Acc@1  98.54 ( 98.14)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:48:15,471 INFO] Epoch: [20][ 40/111]	Loss 0.1453 (0.1495)	InvT  21.52 ( 21.51)	Acc@1  98.05 ( 98.09)	Acc@3  99.90 ( 99.99)
[2025-02-05 13:48:29,520 INFO] Epoch: [20][ 60/111]	Loss 0.1435 (0.1489)	InvT  21.53 ( 21.52)	Acc@1  98.44 ( 98.15)	Acc@3 100.00 ( 99.97)
[2025-02-05 13:48:43,301 INFO] Epoch: [20][ 80/111]	Loss 0.1599 (0.1507)	InvT  21.54 ( 21.52)	Acc@1  97.75 ( 98.12)	Acc@3  99.90 ( 99.97)
[2025-02-05 13:48:57,195 INFO] Epoch: [20][100/111]	Loss 0.1719 (0.1527)	InvT  21.55 ( 21.53)	Acc@1  98.24 ( 98.08)	Acc@3 100.00 ( 99.97)
[2025-02-05 13:49:03,996 INFO] Learning rate: 3.13538045577443e-05
[2025-02-05 13:49:19,742 INFO] Epoch 20, valid metric: {"Acc@1": 8.103, "Acc@3": 18.196, "loss": 6.422}
[2025-02-05 13:49:25,968 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch17.mdl
[2025-02-05 13:49:28,279 INFO] Epoch: [21][  0/111]	Loss 0.1309 (0.1309)	InvT  21.56 ( 21.56)	Acc@1  98.73 ( 98.73)	Acc@3 100.00 (100.00)
[2025-02-05 13:49:42,144 INFO] Epoch: [21][ 20/111]	Loss 0.1414 (0.1328)	InvT  21.57 ( 21.56)	Acc@1  97.95 ( 98.43)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:49:56,289 INFO] Epoch: [21][ 40/111]	Loss 0.1491 (0.134)	InvT  21.58 ( 21.57)	Acc@1  98.14 ( 98.40)	Acc@3  99.90 ( 99.98)
[2025-02-05 13:50:10,394 INFO] Epoch: [21][ 60/111]	Loss 0.1203 (0.1371)	InvT  21.59 ( 21.57)	Acc@1  98.73 ( 98.34)	Acc@3 100.00 ( 99.97)
[2025-02-05 13:50:24,114 INFO] Epoch: [21][ 80/111]	Loss 0.155 (0.1392)	InvT  21.60 ( 21.58)	Acc@1  97.56 ( 98.29)	Acc@3  99.90 ( 99.97)
[2025-02-05 13:50:38,170 INFO] Epoch: [21][100/111]	Loss 0.1633 (0.1416)	InvT  21.61 ( 21.58)	Acc@1  97.75 ( 98.21)	Acc@3  99.90 ( 99.97)
[2025-02-05 13:50:44,967 INFO] Learning rate: 3.0281962147547317e-05
[2025-02-05 13:51:00,677 INFO] Epoch 21, valid metric: {"Acc@1": 8.076, "Acc@3": 18.017, "loss": 6.444}
[2025-02-05 13:51:06,949 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch18.mdl
[2025-02-05 13:51:09,125 INFO] Epoch: [22][  0/111]	Loss 0.1326 (0.1326)	InvT  21.61 ( 21.61)	Acc@1  98.24 ( 98.24)	Acc@3 100.00 (100.00)
[2025-02-05 13:51:23,232 INFO] Epoch: [22][ 20/111]	Loss 0.1326 (0.1328)	InvT  21.62 ( 21.62)	Acc@1  98.73 ( 98.33)	Acc@3 100.00 ( 99.97)
[2025-02-05 13:51:37,403 INFO] Epoch: [22][ 40/111]	Loss 0.1443 (0.13)	InvT  21.63 ( 21.62)	Acc@1  97.95 ( 98.39)	Acc@3 100.00 ( 99.98)
[2025-02-05 13:51:51,321 INFO] Epoch: [22][ 60/111]	Loss 0.1459 (0.1288)	InvT  21.64 ( 21.62)	Acc@1  97.75 ( 98.45)	Acc@3 100.00 ( 99.98)
[2025-02-05 13:52:05,164 INFO] Epoch: [22][ 80/111]	Loss 0.1331 (0.1307)	InvT  21.65 ( 21.63)	Acc@1  98.14 ( 98.39)	Acc@3 100.00 ( 99.98)
[2025-02-05 13:52:19,151 INFO] Epoch: [22][100/111]	Loss 0.1491 (0.1318)	InvT  21.66 ( 21.63)	Acc@1  97.95 ( 98.37)	Acc@3 100.00 ( 99.98)
[2025-02-05 13:52:25,958 INFO] Learning rate: 2.9210119737350328e-05
[2025-02-05 13:52:41,591 INFO] Epoch 22, valid metric: {"Acc@1": 8.107, "Acc@3": 18.347, "loss": 6.493}
[2025-02-05 13:52:47,773 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch19.mdl
[2025-02-05 13:52:50,010 INFO] Epoch: [23][  0/111]	Loss 0.1504 (0.1504)	InvT  21.66 ( 21.66)	Acc@1  98.24 ( 98.24)	Acc@3 100.00 (100.00)
[2025-02-05 13:53:04,073 INFO] Epoch: [23][ 20/111]	Loss 0.1187 (0.1204)	InvT  21.67 ( 21.67)	Acc@1  98.73 ( 98.56)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:53:18,069 INFO] Epoch: [23][ 40/111]	Loss 0.09594 (0.1185)	InvT  21.68 ( 21.67)	Acc@1  99.32 ( 98.56)	Acc@3 100.00 (100.00)
[2025-02-05 13:53:31,974 INFO] Epoch: [23][ 60/111]	Loss 0.1324 (0.1225)	InvT  21.69 ( 21.67)	Acc@1  98.34 ( 98.50)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:53:45,709 INFO] Epoch: [23][ 80/111]	Loss 0.1117 (0.1234)	InvT  21.69 ( 21.68)	Acc@1  98.14 ( 98.47)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:53:59,783 INFO] Epoch: [23][100/111]	Loss 0.1276 (0.1244)	InvT  21.70 ( 21.68)	Acc@1  98.14 ( 98.45)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:54:06,587 INFO] Learning rate: 2.8138277327153345e-05
[2025-02-05 13:54:22,305 INFO] Epoch 23, valid metric: {"Acc@1": 8.191, "Acc@3": 18.142, "loss": 6.443}
[2025-02-05 13:54:31,611 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch20.mdl
[2025-02-05 13:54:33,780 INFO] Epoch: [24][  0/111]	Loss 0.1135 (0.1135)	InvT  21.71 ( 21.71)	Acc@1  98.73 ( 98.73)	Acc@3 100.00 (100.00)
[2025-02-05 13:54:47,829 INFO] Epoch: [24][ 20/111]	Loss 0.08633 (0.1149)	InvT  21.72 ( 21.71)	Acc@1  99.51 ( 98.62)	Acc@3 100.00 (100.00)
[2025-02-05 13:55:01,825 INFO] Epoch: [24][ 40/111]	Loss 0.1102 (0.117)	InvT  21.72 ( 21.72)	Acc@1  98.63 ( 98.56)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:55:15,714 INFO] Epoch: [24][ 60/111]	Loss 0.1144 (0.1174)	InvT  21.73 ( 21.72)	Acc@1  98.83 ( 98.58)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:55:29,508 INFO] Epoch: [24][ 80/111]	Loss 0.1192 (0.117)	InvT  21.74 ( 21.72)	Acc@1  99.12 ( 98.58)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:55:43,249 INFO] Epoch: [24][100/111]	Loss 0.1429 (0.1167)	InvT  21.75 ( 21.73)	Acc@1  97.56 ( 98.58)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:55:50,067 INFO] Learning rate: 2.7066434916956356e-05
[2025-02-05 13:56:05,826 INFO] Epoch 24, valid metric: {"Acc@1": 8.413, "Acc@3": 18.596, "loss": 6.414}
[2025-02-05 13:56:15,303 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch21.mdl
[2025-02-05 13:56:17,465 INFO] Epoch: [25][  0/111]	Loss 0.09006 (0.09006)	InvT  21.75 ( 21.75)	Acc@1  99.12 ( 99.12)	Acc@3 100.00 (100.00)
[2025-02-05 13:56:31,826 INFO] Epoch: [25][ 20/111]	Loss 0.08947 (0.1049)	InvT  21.76 ( 21.76)	Acc@1  99.22 ( 98.80)	Acc@3 100.00 (100.00)
[2025-02-05 13:56:45,650 INFO] Epoch: [25][ 40/111]	Loss 0.09969 (0.1063)	InvT  21.77 ( 21.76)	Acc@1  98.44 ( 98.75)	Acc@3 100.00 ( 99.98)
[2025-02-05 13:56:59,590 INFO] Epoch: [25][ 60/111]	Loss 0.1154 (0.1079)	InvT  21.78 ( 21.77)	Acc@1  98.63 ( 98.77)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:57:13,307 INFO] Epoch: [25][ 80/111]	Loss 0.1284 (0.1088)	InvT  21.79 ( 21.77)	Acc@1  98.24 ( 98.73)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:57:27,300 INFO] Epoch: [25][100/111]	Loss 0.1016 (0.1089)	InvT  21.79 ( 21.77)	Acc@1  98.63 ( 98.73)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:57:34,062 INFO] Learning rate: 2.599459250675937e-05
[2025-02-05 13:57:49,782 INFO] Epoch 25, valid metric: {"Acc@1": 7.494, "Acc@3": 17.156, "loss": 6.643}
[2025-02-05 13:57:56,403 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch22.mdl
[2025-02-05 13:57:58,630 INFO] Epoch: [26][  0/111]	Loss 0.09467 (0.09467)	InvT  21.80 ( 21.80)	Acc@1  98.63 ( 98.63)	Acc@3  99.90 ( 99.90)
[2025-02-05 13:58:12,950 INFO] Epoch: [26][ 20/111]	Loss 0.1007 (0.0968)	InvT  21.80 ( 21.80)	Acc@1  98.73 ( 98.93)	Acc@3  99.90 ( 99.99)
[2025-02-05 13:58:26,704 INFO] Epoch: [26][ 40/111]	Loss 0.1025 (0.09999)	InvT  21.81 ( 21.80)	Acc@1  98.73 ( 98.87)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:58:40,571 INFO] Epoch: [26][ 60/111]	Loss 0.09898 (0.1005)	InvT  21.82 ( 21.81)	Acc@1  98.63 ( 98.87)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:58:54,592 INFO] Epoch: [26][ 80/111]	Loss 0.09896 (0.1012)	InvT  21.83 ( 21.81)	Acc@1  98.54 ( 98.86)	Acc@3  99.90 ( 99.99)
[2025-02-05 13:59:08,331 INFO] Epoch: [26][100/111]	Loss 0.123 (0.1021)	InvT  21.83 ( 21.82)	Acc@1  98.73 ( 98.85)	Acc@3 100.00 ( 99.99)
[2025-02-05 13:59:15,162 INFO] Learning rate: 2.492275009656238e-05
[2025-02-05 13:59:31,030 INFO] Epoch 26, valid metric: {"Acc@1": 7.652, "Acc@3": 17.677, "loss": 6.621}
[2025-02-05 13:59:37,551 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch23.mdl
[2025-02-05 13:59:39,706 INFO] Epoch: [27][  0/111]	Loss 0.08038 (0.08038)	InvT  21.84 ( 21.84)	Acc@1  99.12 ( 99.12)	Acc@3 100.00 (100.00)
[2025-02-05 13:59:53,965 INFO] Epoch: [27][ 20/111]	Loss 0.1066 (0.09467)	InvT  21.85 ( 21.84)	Acc@1  98.83 ( 98.91)	Acc@3 100.00 (100.00)
[2025-02-05 14:00:07,747 INFO] Epoch: [27][ 40/111]	Loss 0.1004 (0.09571)	InvT  21.85 ( 21.85)	Acc@1  99.12 ( 98.92)	Acc@3 100.00 (100.00)
[2025-02-05 14:00:21,743 INFO] Epoch: [27][ 60/111]	Loss 0.09528 (0.09556)	InvT  21.86 ( 21.85)	Acc@1  98.83 ( 98.94)	Acc@3 100.00 (100.00)
[2025-02-05 14:00:35,701 INFO] Epoch: [27][ 80/111]	Loss 0.08124 (0.0951)	InvT  21.87 ( 21.85)	Acc@1  99.51 ( 98.94)	Acc@3 100.00 (100.00)
[2025-02-05 14:00:49,434 INFO] Epoch: [27][100/111]	Loss 0.1042 (0.0967)	InvT  21.87 ( 21.86)	Acc@1  99.02 ( 98.90)	Acc@3 100.00 (100.00)
[2025-02-05 14:00:56,217 INFO] Learning rate: 2.385090768636539e-05
[2025-02-05 14:01:11,902 INFO] Epoch 27, valid metric: {"Acc@1": 8.029, "Acc@3": 18.101, "loss": 6.535}
[2025-02-05 14:01:19,267 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch24.mdl
[2025-02-05 14:01:21,509 INFO] Epoch: [28][  0/111]	Loss 0.1009 (0.1009)	InvT  21.88 ( 21.88)	Acc@1  98.73 ( 98.73)	Acc@3 100.00 (100.00)
[2025-02-05 14:01:35,672 INFO] Epoch: [28][ 20/111]	Loss 0.0936 (0.0945)	InvT  21.88 ( 21.88)	Acc@1  99.02 ( 98.87)	Acc@3 100.00 (100.00)
[2025-02-05 14:01:49,407 INFO] Epoch: [28][ 40/111]	Loss 0.0906 (0.09349)	InvT  21.89 ( 21.88)	Acc@1  99.02 ( 98.88)	Acc@3 100.00 (100.00)
[2025-02-05 14:02:03,427 INFO] Epoch: [28][ 60/111]	Loss 0.0927 (0.09291)	InvT  21.90 ( 21.89)	Acc@1  99.02 ( 98.89)	Acc@3 100.00 (100.00)
[2025-02-05 14:02:17,214 INFO] Epoch: [28][ 80/111]	Loss 0.08202 (0.09261)	InvT  21.90 ( 21.89)	Acc@1  99.12 ( 98.92)	Acc@3 100.00 (100.00)
[2025-02-05 14:02:31,167 INFO] Epoch: [28][100/111]	Loss 0.09045 (0.09262)	InvT  21.91 ( 21.89)	Acc@1  99.02 ( 98.93)	Acc@3 100.00 (100.00)
[2025-02-05 14:02:37,953 INFO] Learning rate: 2.2779065276168405e-05
[2025-02-05 14:02:53,894 INFO] Epoch 28, valid metric: {"Acc@1": 7.642, "Acc@3": 17.401, "loss": 6.594}
[2025-02-05 14:03:00,164 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch25.mdl
[2025-02-05 14:03:02,393 INFO] Epoch: [29][  0/111]	Loss 0.08544 (0.08544)	InvT  21.92 ( 21.92)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
[2025-02-05 14:03:16,493 INFO] Epoch: [29][ 20/111]	Loss 0.08052 (0.08333)	InvT  21.92 ( 21.92)	Acc@1  99.12 ( 99.07)	Acc@3 100.00 ( 99.99)
[2025-02-05 14:03:30,254 INFO] Epoch: [29][ 40/111]	Loss 0.06619 (0.08423)	InvT  21.93 ( 21.92)	Acc@1  99.71 ( 99.09)	Acc@3 100.00 (100.00)
[2025-02-05 14:03:44,217 INFO] Epoch: [29][ 60/111]	Loss 0.07832 (0.08566)	InvT  21.93 ( 21.93)	Acc@1  99.22 ( 99.03)	Acc@3 100.00 ( 99.99)
[2025-02-05 14:03:58,136 INFO] Epoch: [29][ 80/111]	Loss 0.1141 (0.08636)	InvT  21.94 ( 21.93)	Acc@1  98.63 ( 99.02)	Acc@3  99.90 ( 99.99)
[2025-02-05 14:04:11,958 INFO] Epoch: [29][100/111]	Loss 0.09028 (0.08701)	InvT  21.95 ( 21.93)	Acc@1  99.22 ( 99.02)	Acc@3 100.00 ( 99.99)
[2025-02-05 14:04:18,802 INFO] Learning rate: 2.170722286597142e-05
[2025-02-05 14:04:34,123 INFO] Epoch 29, valid metric: {"Acc@1": 8.649, "Acc@3": 18.593, "loss": 6.509}
[2025-02-05 14:04:46,379 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch26.mdl
[2025-02-05 14:04:48,660 INFO] Epoch: [30][  0/111]	Loss 0.06961 (0.06961)	InvT  21.95 ( 21.95)	Acc@1  99.41 ( 99.41)	Acc@3 100.00 (100.00)
[2025-02-05 14:05:02,883 INFO] Epoch: [30][ 20/111]	Loss 0.06969 (0.08224)	InvT  21.96 ( 21.95)	Acc@1  99.51 ( 99.12)	Acc@3 100.00 ( 99.99)
[2025-02-05 14:05:16,708 INFO] Epoch: [30][ 40/111]	Loss 0.07122 (0.08191)	InvT  21.96 ( 21.96)	Acc@1  99.32 ( 99.09)	Acc@3 100.00 ( 99.99)
[2025-02-05 14:05:30,541 INFO] Epoch: [30][ 60/111]	Loss 0.07328 (0.08177)	InvT  21.97 ( 21.96)	Acc@1  99.41 ( 99.08)	Acc@3 100.00 (100.00)
[2025-02-05 14:05:44,575 INFO] Epoch: [30][ 80/111]	Loss 0.08414 (0.08298)	InvT  21.98 ( 21.96)	Acc@1  98.73 ( 99.05)	Acc@3 100.00 (100.00)
[2025-02-05 14:05:58,478 INFO] Epoch: [30][100/111]	Loss 0.06347 (0.08271)	InvT  21.98 ( 21.97)	Acc@1  99.51 ( 99.07)	Acc@3 100.00 (100.00)
[2025-02-05 14:06:05,432 INFO] Learning rate: 2.063538045577443e-05
[2025-02-05 14:06:20,837 INFO] Epoch 30, valid metric: {"Acc@1": 8.09, "Acc@3": 18.031, "loss": 6.535}
[2025-02-05 14:06:27,182 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch27.mdl
[2025-02-05 14:06:29,338 INFO] Epoch: [31][  0/111]	Loss 0.0653 (0.0653)	InvT  21.99 ( 21.99)	Acc@1  99.32 ( 99.32)	Acc@3 100.00 (100.00)
[2025-02-05 14:06:43,444 INFO] Epoch: [31][ 20/111]	Loss 0.069 (0.07359)	InvT  21.99 ( 21.99)	Acc@1  99.61 ( 99.27)	Acc@3 100.00 (100.00)
[2025-02-05 14:06:57,158 INFO] Epoch: [31][ 40/111]	Loss 0.06539 (0.07535)	InvT  22.00 ( 21.99)	Acc@1  99.41 ( 99.21)	Acc@3 100.00 (100.00)
[2025-02-05 14:07:11,182 INFO] Epoch: [31][ 60/111]	Loss 0.07319 (0.07706)	InvT  22.00 ( 21.99)	Acc@1  99.41 ( 99.22)	Acc@3 100.00 (100.00)
[2025-02-05 14:07:25,053 INFO] Epoch: [31][ 80/111]	Loss 0.1031 (0.07915)	InvT  22.01 ( 22.00)	Acc@1  98.73 ( 99.17)	Acc@3 100.00 (100.00)
[2025-02-05 14:07:38,868 INFO] Epoch: [31][100/111]	Loss 0.07561 (0.07921)	InvT  22.01 ( 22.00)	Acc@1  99.22 ( 99.17)	Acc@3 100.00 (100.00)
[2025-02-05 14:07:45,646 INFO] Learning rate: 1.9563538045577444e-05
[2025-02-05 14:08:01,716 INFO] Epoch 31, valid metric: {"Acc@1": 8.11, "Acc@3": 17.862, "loss": 6.614}
[2025-02-05 14:08:07,848 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch28.mdl
[2025-02-05 14:08:10,097 INFO] Epoch: [32][  0/111]	Loss 0.076 (0.076)	InvT  22.02 ( 22.02)	Acc@1  99.32 ( 99.32)	Acc@3 100.00 (100.00)
[2025-02-05 14:08:24,482 INFO] Epoch: [32][ 20/111]	Loss 0.06619 (0.07413)	InvT  22.02 ( 22.02)	Acc@1  99.51 ( 99.21)	Acc@3 100.00 (100.00)
[2025-02-05 14:08:38,231 INFO] Epoch: [32][ 40/111]	Loss 0.06945 (0.07321)	InvT  22.03 ( 22.02)	Acc@1  99.22 ( 99.19)	Acc@3 100.00 (100.00)
[2025-02-05 14:08:52,240 INFO] Epoch: [32][ 60/111]	Loss 0.05855 (0.0737)	InvT  22.03 ( 22.03)	Acc@1  99.61 ( 99.18)	Acc@3 100.00 (100.00)
[2025-02-05 14:09:05,991 INFO] Epoch: [32][ 80/111]	Loss 0.06105 (0.07372)	InvT  22.04 ( 22.03)	Acc@1  99.51 ( 99.18)	Acc@3 100.00 (100.00)
[2025-02-05 14:09:19,820 INFO] Epoch: [32][100/111]	Loss 0.1043 (0.07479)	InvT  22.04 ( 22.03)	Acc@1  98.73 ( 99.15)	Acc@3  99.90 (100.00)
[2025-02-05 14:09:26,643 INFO] Learning rate: 1.8491695635380458e-05
[2025-02-05 14:09:42,532 INFO] Epoch 32, valid metric: {"Acc@1": 7.824, "Acc@3": 17.634, "loss": 6.62}
[2025-02-05 14:09:49,210 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch29.mdl
[2025-02-05 14:09:51,371 INFO] Epoch: [33][  0/111]	Loss 0.07981 (0.07981)	InvT  22.05 ( 22.05)	Acc@1  99.12 ( 99.12)	Acc@3 100.00 (100.00)
[2025-02-05 14:10:05,459 INFO] Epoch: [33][ 20/111]	Loss 0.06959 (0.06934)	InvT  22.05 ( 22.05)	Acc@1  99.71 ( 99.33)	Acc@3 100.00 (100.00)
[2025-02-05 14:10:19,563 INFO] Epoch: [33][ 40/111]	Loss 0.07683 (0.0702)	InvT  22.06 ( 22.05)	Acc@1  99.41 ( 99.32)	Acc@3 100.00 (100.00)
[2025-02-05 14:10:33,592 INFO] Epoch: [33][ 60/111]	Loss 0.08042 (0.07175)	InvT  22.06 ( 22.06)	Acc@1  98.93 ( 99.26)	Acc@3 100.00 (100.00)
[2025-02-05 14:10:47,421 INFO] Epoch: [33][ 80/111]	Loss 0.07372 (0.07128)	InvT  22.07 ( 22.06)	Acc@1  99.22 ( 99.25)	Acc@3 100.00 (100.00)
[2025-02-05 14:11:01,431 INFO] Epoch: [33][100/111]	Loss 0.07951 (0.07234)	InvT  22.07 ( 22.06)	Acc@1  98.83 ( 99.22)	Acc@3 100.00 (100.00)
[2025-02-05 14:11:08,241 INFO] Learning rate: 1.741985322518347e-05
[2025-02-05 14:11:24,021 INFO] Epoch 33, valid metric: {"Acc@1": 8.214, "Acc@3": 18.478, "loss": 6.504}
[2025-02-05 14:11:30,133 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch30.mdl
[2025-02-05 14:11:32,417 INFO] Epoch: [34][  0/111]	Loss 0.07842 (0.07842)	InvT  22.08 ( 22.08)	Acc@1  98.93 ( 98.93)	Acc@3 100.00 (100.00)
[2025-02-05 14:11:46,317 INFO] Epoch: [34][ 20/111]	Loss 0.07107 (0.06921)	InvT  22.08 ( 22.08)	Acc@1  99.22 ( 99.18)	Acc@3 100.00 (100.00)
[2025-02-05 14:12:00,432 INFO] Epoch: [34][ 40/111]	Loss 0.05552 (0.06939)	InvT  22.09 ( 22.08)	Acc@1  99.51 ( 99.22)	Acc@3 100.00 (100.00)
[2025-02-05 14:12:14,273 INFO] Epoch: [34][ 60/111]	Loss 0.07971 (0.06801)	InvT  22.09 ( 22.08)	Acc@1  98.93 ( 99.25)	Acc@3 100.00 (100.00)
[2025-02-05 14:12:28,280 INFO] Epoch: [34][ 80/111]	Loss 0.06755 (0.06822)	InvT  22.10 ( 22.09)	Acc@1  99.12 ( 99.28)	Acc@3 100.00 (100.00)
[2025-02-05 14:12:42,289 INFO] Epoch: [34][100/111]	Loss 0.05837 (0.06866)	InvT  22.10 ( 22.09)	Acc@1  99.51 ( 99.26)	Acc@3 100.00 (100.00)
[2025-02-05 14:12:49,131 INFO] Learning rate: 1.634801081498648e-05
[2025-02-05 14:13:04,378 INFO] Epoch 34, valid metric: {"Acc@1": 7.912, "Acc@3": 18.111, "loss": 6.597}
[2025-02-05 14:13:10,244 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch31.mdl
[2025-02-05 14:13:12,507 INFO] Epoch: [35][  0/111]	Loss 0.06264 (0.06264)	InvT  22.10 ( 22.10)	Acc@1  99.32 ( 99.32)	Acc@3 100.00 (100.00)
[2025-02-05 14:13:26,888 INFO] Epoch: [35][ 20/111]	Loss 0.0641 (0.0673)	InvT  22.11 ( 22.11)	Acc@1  99.71 ( 99.36)	Acc@3 100.00 (100.00)
[2025-02-05 14:13:40,823 INFO] Epoch: [35][ 40/111]	Loss 0.07426 (0.06705)	InvT  22.11 ( 22.11)	Acc@1  99.12 ( 99.32)	Acc@3 100.00 (100.00)
[2025-02-05 14:13:54,618 INFO] Epoch: [35][ 60/111]	Loss 0.07569 (0.06711)	InvT  22.12 ( 22.11)	Acc@1  98.93 ( 99.31)	Acc@3 100.00 (100.00)
[2025-02-05 14:14:08,488 INFO] Epoch: [35][ 80/111]	Loss 0.05948 (0.06699)	InvT  22.12 ( 22.11)	Acc@1  99.61 ( 99.31)	Acc@3 100.00 (100.00)
[2025-02-05 14:14:22,547 INFO] Epoch: [35][100/111]	Loss 0.06578 (0.06698)	InvT  22.13 ( 22.12)	Acc@1  99.12 ( 99.31)	Acc@3 100.00 (100.00)
[2025-02-05 14:14:29,329 INFO] Learning rate: 1.5276168404789497e-05
[2025-02-05 14:14:44,823 INFO] Epoch 35, valid metric: {"Acc@1": 8.272, "Acc@3": 18.135, "loss": 6.554}
[2025-02-05 14:14:51,084 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch32.mdl
[2025-02-05 14:14:53,372 INFO] Epoch: [36][  0/111]	Loss 0.06511 (0.06511)	InvT  22.13 ( 22.13)	Acc@1  98.93 ( 98.93)	Acc@3 100.00 (100.00)
[2025-02-05 14:15:07,355 INFO] Epoch: [36][ 20/111]	Loss 0.05746 (0.06347)	InvT  22.13 ( 22.13)	Acc@1  99.41 ( 99.32)	Acc@3 100.00 (100.00)
[2025-02-05 14:15:21,473 INFO] Epoch: [36][ 40/111]	Loss 0.07516 (0.06347)	InvT  22.14 ( 22.13)	Acc@1  99.32 ( 99.32)	Acc@3 100.00 (100.00)
[2025-02-05 14:15:35,455 INFO] Epoch: [36][ 60/111]	Loss 0.06809 (0.06266)	InvT  22.14 ( 22.14)	Acc@1  99.12 ( 99.32)	Acc@3 100.00 (100.00)
[2025-02-05 14:15:49,287 INFO] Epoch: [36][ 80/111]	Loss 0.04548 (0.06339)	InvT  22.15 ( 22.14)	Acc@1  99.80 ( 99.33)	Acc@3 100.00 (100.00)
[2025-02-05 14:16:03,172 INFO] Epoch: [36][100/111]	Loss 0.06083 (0.06367)	InvT  22.15 ( 22.14)	Acc@1  99.12 ( 99.32)	Acc@3 100.00 (100.00)
[2025-02-05 14:16:09,953 INFO] Learning rate: 1.4204325994592508e-05
[2025-02-05 14:16:25,618 INFO] Epoch 36, valid metric: {"Acc@1": 8.544, "Acc@3": 18.317, "loss": 6.609}
[2025-02-05 14:16:31,679 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch33.mdl
[2025-02-05 14:16:33,953 INFO] Epoch: [37][  0/111]	Loss 0.0722 (0.0722)	InvT  22.15 ( 22.15)	Acc@1  98.93 ( 98.93)	Acc@3 100.00 (100.00)
[2025-02-05 14:16:47,998 INFO] Epoch: [37][ 20/111]	Loss 0.05557 (0.06223)	InvT  22.16 ( 22.16)	Acc@1  99.32 ( 99.31)	Acc@3 100.00 (100.00)
[2025-02-05 14:17:01,853 INFO] Epoch: [37][ 40/111]	Loss 0.07626 (0.06008)	InvT  22.16 ( 22.16)	Acc@1  98.54 ( 99.35)	Acc@3 100.00 (100.00)
[2025-02-05 14:17:15,720 INFO] Epoch: [37][ 60/111]	Loss 0.06071 (0.06029)	InvT  22.17 ( 22.16)	Acc@1  99.51 ( 99.37)	Acc@3 100.00 (100.00)
[2025-02-05 14:17:29,785 INFO] Epoch: [37][ 80/111]	Loss 0.04787 (0.06045)	InvT  22.17 ( 22.16)	Acc@1  99.71 ( 99.38)	Acc@3 100.00 (100.00)
[2025-02-05 14:17:43,730 INFO] Epoch: [37][100/111]	Loss 0.06312 (0.06043)	InvT  22.17 ( 22.16)	Acc@1  99.41 ( 99.38)	Acc@3 100.00 (100.00)
[2025-02-05 14:17:50,547 INFO] Learning rate: 1.3132483584395518e-05
[2025-02-05 14:18:06,279 INFO] Epoch 37, valid metric: {"Acc@1": 7.905, "Acc@3": 17.354, "loss": 6.675}
[2025-02-05 14:18:15,473 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch34.mdl
[2025-02-05 14:18:17,682 INFO] Epoch: [38][  0/111]	Loss 0.064 (0.064)	InvT  22.18 ( 22.18)	Acc@1  99.12 ( 99.12)	Acc@3 100.00 (100.00)
[2025-02-05 14:18:31,709 INFO] Epoch: [38][ 20/111]	Loss 0.0467 (0.05873)	InvT  22.18 ( 22.18)	Acc@1  99.90 ( 99.39)	Acc@3 100.00 (100.00)
[2025-02-05 14:18:45,749 INFO] Epoch: [38][ 40/111]	Loss 0.05754 (0.0592)	InvT  22.18 ( 22.18)	Acc@1  99.61 ( 99.36)	Acc@3 100.00 (100.00)
[2025-02-05 14:18:59,802 INFO] Epoch: [38][ 60/111]	Loss 0.05856 (0.0598)	InvT  22.19 ( 22.18)	Acc@1  99.51 ( 99.36)	Acc@3 100.00 (100.00)
[2025-02-05 14:19:13,705 INFO] Epoch: [38][ 80/111]	Loss 0.05828 (0.05965)	InvT  22.19 ( 22.18)	Acc@1  99.61 ( 99.37)	Acc@3 100.00 (100.00)
[2025-02-05 14:19:27,687 INFO] Epoch: [38][100/111]	Loss 0.07133 (0.06011)	InvT  22.19 ( 22.19)	Acc@1  98.83 ( 99.36)	Acc@3 100.00 (100.00)
[2025-02-05 14:19:34,495 INFO] Learning rate: 1.2060641174198533e-05
[2025-02-05 14:19:50,221 INFO] Epoch 38, valid metric: {"Acc@1": 7.905, "Acc@3": 17.277, "loss": 6.716}
[2025-02-05 14:19:55,649 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch35.mdl
[2025-02-05 14:19:57,826 INFO] Epoch: [39][  0/111]	Loss 0.0472 (0.0472)	InvT  22.20 ( 22.20)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2025-02-05 14:20:11,826 INFO] Epoch: [39][ 20/111]	Loss 0.05123 (0.05644)	InvT  22.20 ( 22.20)	Acc@1  99.90 ( 99.51)	Acc@3 100.00 (100.00)
[2025-02-05 14:20:25,701 INFO] Epoch: [39][ 40/111]	Loss 0.05131 (0.0551)	InvT  22.20 ( 22.20)	Acc@1  99.41 ( 99.49)	Acc@3 100.00 (100.00)
[2025-02-05 14:20:39,646 INFO] Epoch: [39][ 60/111]	Loss 0.05269 (0.05597)	InvT  22.21 ( 22.20)	Acc@1  99.51 ( 99.46)	Acc@3 100.00 (100.00)
[2025-02-05 14:20:53,543 INFO] Epoch: [39][ 80/111]	Loss 0.05707 (0.05682)	InvT  22.21 ( 22.20)	Acc@1  99.32 ( 99.43)	Acc@3 100.00 (100.00)
[2025-02-05 14:21:07,554 INFO] Epoch: [39][100/111]	Loss 0.04553 (0.05696)	InvT  22.21 ( 22.21)	Acc@1  99.51 ( 99.41)	Acc@3 100.00 (100.00)
[2025-02-05 14:21:14,348 INFO] Learning rate: 1.0988798764001545e-05
[2025-02-05 14:21:30,054 INFO] Epoch 39, valid metric: {"Acc@1": 7.976, "Acc@3": 17.899, "loss": 6.672}
[2025-02-05 14:21:36,167 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch36.mdl
[2025-02-05 14:21:38,428 INFO] Epoch: [40][  0/111]	Loss 0.0538 (0.0538)	InvT  22.22 ( 22.22)	Acc@1  99.41 ( 99.41)	Acc@3 100.00 (100.00)
[2025-02-05 14:21:52,487 INFO] Epoch: [40][ 20/111]	Loss 0.06175 (0.05563)	InvT  22.22 ( 22.22)	Acc@1  99.32 ( 99.43)	Acc@3 100.00 (100.00)
[2025-02-05 14:22:06,365 INFO] Epoch: [40][ 40/111]	Loss 0.05409 (0.05482)	InvT  22.22 ( 22.22)	Acc@1  99.41 ( 99.42)	Acc@3 100.00 (100.00)
[2025-02-05 14:22:20,316 INFO] Epoch: [40][ 60/111]	Loss 0.04466 (0.05546)	InvT  22.23 ( 22.22)	Acc@1  99.80 ( 99.43)	Acc@3 100.00 (100.00)
[2025-02-05 14:22:34,040 INFO] Epoch: [40][ 80/111]	Loss 0.04754 (0.05468)	InvT  22.23 ( 22.22)	Acc@1  99.71 ( 99.44)	Acc@3 100.00 (100.00)
[2025-02-05 14:22:48,020 INFO] Epoch: [40][100/111]	Loss 0.04525 (0.05529)	InvT  22.23 ( 22.22)	Acc@1  99.71 ( 99.44)	Acc@3 100.00 (100.00)
[2025-02-05 14:22:54,796 INFO] Learning rate: 9.916956353804559e-06
[2025-02-05 14:23:10,460 INFO] Epoch 40, valid metric: {"Acc@1": 8.356, "Acc@3": 18.401, "loss": 6.628}
[2025-02-05 14:23:17,232 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch37.mdl
[2025-02-05 14:23:19,484 INFO] Epoch: [41][  0/111]	Loss 0.04995 (0.04995)	InvT  22.23 ( 22.23)	Acc@1  99.41 ( 99.41)	Acc@3 100.00 (100.00)
[2025-02-05 14:23:33,469 INFO] Epoch: [41][ 20/111]	Loss 0.04963 (0.05083)	InvT  22.24 ( 22.24)	Acc@1  99.51 ( 99.55)	Acc@3 100.00 (100.00)
[2025-02-05 14:23:47,290 INFO] Epoch: [41][ 40/111]	Loss 0.05361 (0.05278)	InvT  22.24 ( 22.24)	Acc@1  99.32 ( 99.50)	Acc@3 100.00 (100.00)
[2025-02-05 14:24:01,234 INFO] Epoch: [41][ 60/111]	Loss 0.05521 (0.05249)	InvT  22.24 ( 22.24)	Acc@1  99.51 ( 99.50)	Acc@3 100.00 (100.00)
[2025-02-05 14:24:14,957 INFO] Epoch: [41][ 80/111]	Loss 0.05204 (0.05224)	InvT  22.25 ( 22.24)	Acc@1  99.32 ( 99.50)	Acc@3  99.90 (100.00)
[2025-02-05 14:24:28,729 INFO] Epoch: [41][100/111]	Loss 0.06919 (0.05244)	InvT  22.25 ( 22.24)	Acc@1  98.83 ( 99.49)	Acc@3 100.00 (100.00)
[2025-02-05 14:24:35,504 INFO] Learning rate: 8.845113943607571e-06
[2025-02-05 14:24:51,205 INFO] Epoch 41, valid metric: {"Acc@1": 8.15, "Acc@3": 18.031, "loss": 6.64}
[2025-02-05 14:24:58,993 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch38.mdl
[2025-02-05 14:25:01,120 INFO] Epoch: [42][  0/111]	Loss 0.04359 (0.04359)	InvT  22.25 ( 22.25)	Acc@1  99.71 ( 99.71)	Acc@3 100.00 (100.00)
[2025-02-05 14:25:15,097 INFO] Epoch: [42][ 20/111]	Loss 0.05177 (0.05224)	InvT  22.25 ( 22.25)	Acc@1  99.41 ( 99.41)	Acc@3 100.00 (100.00)
[2025-02-05 14:25:29,057 INFO] Epoch: [42][ 40/111]	Loss 0.05385 (0.05325)	InvT  22.25 ( 22.25)	Acc@1  99.22 ( 99.43)	Acc@3 100.00 (100.00)
[2025-02-05 14:25:43,127 INFO] Epoch: [42][ 60/111]	Loss 0.05045 (0.05193)	InvT  22.26 ( 22.25)	Acc@1  99.61 ( 99.47)	Acc@3 100.00 (100.00)
[2025-02-05 14:25:56,894 INFO] Epoch: [42][ 80/111]	Loss 0.07108 (0.05201)	InvT  22.26 ( 22.25)	Acc@1  99.32 ( 99.47)	Acc@3 100.00 (100.00)
[2025-02-05 14:26:10,856 INFO] Epoch: [42][100/111]	Loss 0.05477 (0.05202)	InvT  22.26 ( 22.26)	Acc@1  99.41 ( 99.48)	Acc@3 100.00 (100.00)
[2025-02-05 14:26:17,651 INFO] Learning rate: 7.773271533410584e-06
[2025-02-05 14:26:33,265 INFO] Epoch 42, valid metric: {"Acc@1": 8.029, "Acc@3": 17.681, "loss": 6.708}
[2025-02-05 14:26:42,161 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch39.mdl
[2025-02-05 14:26:44,328 INFO] Epoch: [43][  0/111]	Loss 0.0536 (0.0536)	InvT  22.26 ( 22.26)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2025-02-05 14:26:58,358 INFO] Epoch: [43][ 20/111]	Loss 0.04288 (0.04756)	InvT  22.27 ( 22.26)	Acc@1  99.90 ( 99.57)	Acc@3 100.00 (100.00)
[2025-02-05 14:27:12,567 INFO] Epoch: [43][ 40/111]	Loss 0.04324 (0.04868)	InvT  22.27 ( 22.27)	Acc@1  99.51 ( 99.54)	Acc@3 100.00 (100.00)
[2025-02-05 14:27:26,378 INFO] Epoch: [43][ 60/111]	Loss 0.04825 (0.04863)	InvT  22.27 ( 22.27)	Acc@1  99.71 ( 99.55)	Acc@3 100.00 (100.00)
[2025-02-05 14:27:40,274 INFO] Epoch: [43][ 80/111]	Loss 0.05557 (0.04872)	InvT  22.27 ( 22.27)	Acc@1  99.22 ( 99.54)	Acc@3 100.00 (100.00)
[2025-02-05 14:27:54,057 INFO] Epoch: [43][100/111]	Loss 0.05416 (0.04875)	InvT  22.27 ( 22.27)	Acc@1  99.61 ( 99.54)	Acc@3 100.00 (100.00)
[2025-02-05 14:28:00,823 INFO] Learning rate: 6.701429123213595e-06
[2025-02-05 14:28:16,438 INFO] Epoch 43, valid metric: {"Acc@1": 7.959, "Acc@3": 18.179, "loss": 6.669}
[2025-02-05 14:28:22,126 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch40.mdl
[2025-02-05 14:28:24,326 INFO] Epoch: [44][  0/111]	Loss 0.04102 (0.04102)	InvT  22.28 ( 22.28)	Acc@1  99.41 ( 99.41)	Acc@3 100.00 (100.00)
[2025-02-05 14:28:38,341 INFO] Epoch: [44][ 20/111]	Loss 0.05419 (0.04794)	InvT  22.28 ( 22.28)	Acc@1  99.41 ( 99.56)	Acc@3 100.00 (100.00)
[2025-02-05 14:28:52,241 INFO] Epoch: [44][ 40/111]	Loss 0.04653 (0.04709)	InvT  22.28 ( 22.28)	Acc@1  99.61 ( 99.57)	Acc@3 100.00 (100.00)
[2025-02-05 14:29:06,054 INFO] Epoch: [44][ 60/111]	Loss 0.0463 (0.04697)	InvT  22.28 ( 22.28)	Acc@1  99.51 ( 99.58)	Acc@3 100.00 (100.00)
[2025-02-05 14:29:19,888 INFO] Epoch: [44][ 80/111]	Loss 0.03952 (0.0476)	InvT  22.28 ( 22.28)	Acc@1  99.61 ( 99.56)	Acc@3 100.00 (100.00)
[2025-02-05 14:29:33,917 INFO] Epoch: [44][100/111]	Loss 0.05499 (0.04764)	InvT  22.29 ( 22.28)	Acc@1  99.51 ( 99.56)	Acc@3 100.00 (100.00)
[2025-02-05 14:29:40,689 INFO] Learning rate: 5.6295867130166085e-06
[2025-02-05 14:29:56,414 INFO] Epoch 44, valid metric: {"Acc@1": 8.127, "Acc@3": 17.442, "loss": 6.714}
[2025-02-05 14:30:02,200 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch41.mdl
[2025-02-05 14:30:04,366 INFO] Epoch: [45][  0/111]	Loss 0.05824 (0.05824)	InvT  22.29 ( 22.29)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2025-02-05 14:30:18,326 INFO] Epoch: [45][ 20/111]	Loss 0.05561 (0.04649)	InvT  22.29 ( 22.29)	Acc@1  99.51 ( 99.55)	Acc@3 100.00 (100.00)
[2025-02-05 14:30:32,378 INFO] Epoch: [45][ 40/111]	Loss 0.03342 (0.04747)	InvT  22.29 ( 22.29)	Acc@1  99.80 ( 99.54)	Acc@3 100.00 (100.00)
[2025-02-05 14:30:46,320 INFO] Epoch: [45][ 60/111]	Loss 0.03892 (0.04783)	InvT  22.29 ( 22.29)	Acc@1  99.80 ( 99.54)	Acc@3 100.00 (100.00)
[2025-02-05 14:31:00,113 INFO] Epoch: [45][ 80/111]	Loss 0.03895 (0.04806)	InvT  22.29 ( 22.29)	Acc@1  99.61 ( 99.53)	Acc@3 100.00 (100.00)
[2025-02-05 14:31:14,096 INFO] Epoch: [45][100/111]	Loss 0.05112 (0.04854)	InvT  22.29 ( 22.29)	Acc@1  99.61 ( 99.52)	Acc@3 100.00 (100.00)
[2025-02-05 14:31:20,886 INFO] Learning rate: 4.557744302819622e-06
[2025-02-05 14:31:36,579 INFO] Epoch 45, valid metric: {"Acc@1": 7.851, "Acc@3": 17.59, "loss": 6.682}
[2025-02-05 14:31:43,348 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch42.mdl
[2025-02-05 14:31:45,537 INFO] Epoch: [46][  0/111]	Loss 0.03808 (0.03808)	InvT  22.30 ( 22.30)	Acc@1  99.71 ( 99.71)	Acc@3 100.00 (100.00)
[2025-02-05 14:31:59,687 INFO] Epoch: [46][ 20/111]	Loss 0.05183 (0.04562)	InvT  22.30 ( 22.30)	Acc@1  99.71 ( 99.55)	Acc@3 100.00 (100.00)
[2025-02-05 14:32:13,596 INFO] Epoch: [46][ 40/111]	Loss 0.05099 (0.04687)	InvT  22.30 ( 22.30)	Acc@1  99.41 ( 99.51)	Acc@3 100.00 (100.00)
[2025-02-05 14:32:27,408 INFO] Epoch: [46][ 60/111]	Loss 0.05379 (0.0479)	InvT  22.30 ( 22.30)	Acc@1  99.32 ( 99.48)	Acc@3 100.00 (100.00)
[2025-02-05 14:32:41,141 INFO] Epoch: [46][ 80/111]	Loss 0.04455 (0.04798)	InvT  22.30 ( 22.30)	Acc@1  99.71 ( 99.50)	Acc@3 100.00 (100.00)
[2025-02-05 14:32:55,222 INFO] Epoch: [46][100/111]	Loss 0.05403 (0.04781)	InvT  22.30 ( 22.30)	Acc@1  99.22 ( 99.53)	Acc@3 100.00 (100.00)
[2025-02-05 14:33:02,005 INFO] Learning rate: 3.4859018926226345e-06
[2025-02-05 14:33:17,709 INFO] Epoch 46, valid metric: {"Acc@1": 7.885, "Acc@3": 17.274, "loss": 6.72}
[2025-02-05 14:33:24,082 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch43.mdl
[2025-02-05 14:33:26,345 INFO] Epoch: [47][  0/111]	Loss 0.03905 (0.03905)	InvT  22.30 ( 22.30)	Acc@1  99.71 ( 99.71)	Acc@3 100.00 (100.00)
[2025-02-05 14:33:40,457 INFO] Epoch: [47][ 20/111]	Loss 0.03915 (0.04478)	InvT  22.30 ( 22.30)	Acc@1  99.51 ( 99.57)	Acc@3 100.00 (100.00)
[2025-02-05 14:33:54,240 INFO] Epoch: [47][ 40/111]	Loss 0.05457 (0.04527)	InvT  22.30 ( 22.30)	Acc@1  99.51 ( 99.55)	Acc@3 100.00 (100.00)
[2025-02-05 14:34:08,182 INFO] Epoch: [47][ 60/111]	Loss 0.03989 (0.04468)	InvT  22.31 ( 22.30)	Acc@1  99.80 ( 99.59)	Acc@3 100.00 (100.00)
[2025-02-05 14:34:21,940 INFO] Epoch: [47][ 80/111]	Loss 0.0416 (0.04477)	InvT  22.31 ( 22.30)	Acc@1  99.51 ( 99.58)	Acc@3 100.00 (100.00)
[2025-02-05 14:34:35,872 INFO] Epoch: [47][100/111]	Loss 0.04753 (0.04466)	InvT  22.31 ( 22.31)	Acc@1  99.61 ( 99.60)	Acc@3 100.00 (100.00)
[2025-02-05 14:34:42,703 INFO] Learning rate: 2.4140594824256473e-06
[2025-02-05 14:34:58,273 INFO] Epoch 47, valid metric: {"Acc@1": 8.036, "Acc@3": 17.56, "loss": 6.711}
[2025-02-05 14:35:04,748 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch44.mdl
[2025-02-05 14:35:06,929 INFO] Epoch: [48][  0/111]	Loss 0.04599 (0.04599)	InvT  22.31 ( 22.31)	Acc@1  99.02 ( 99.02)	Acc@3 100.00 (100.00)
[2025-02-05 14:35:21,068 INFO] Epoch: [48][ 20/111]	Loss 0.04907 (0.04411)	InvT  22.31 ( 22.31)	Acc@1  99.51 ( 99.57)	Acc@3 100.00 (100.00)
[2025-02-05 14:35:34,930 INFO] Epoch: [48][ 40/111]	Loss 0.04171 (0.04461)	InvT  22.31 ( 22.31)	Acc@1  99.61 ( 99.58)	Acc@3 100.00 (100.00)
[2025-02-05 14:35:48,758 INFO] Epoch: [48][ 60/111]	Loss 0.04065 (0.04383)	InvT  22.31 ( 22.31)	Acc@1  99.61 ( 99.60)	Acc@3 100.00 (100.00)
[2025-02-05 14:36:02,536 INFO] Epoch: [48][ 80/111]	Loss 0.03576 (0.04368)	InvT  22.31 ( 22.31)	Acc@1  99.90 ( 99.60)	Acc@3 100.00 (100.00)
[2025-02-05 14:36:16,454 INFO] Epoch: [48][100/111]	Loss 0.04814 (0.04385)	InvT  22.31 ( 22.31)	Acc@1  99.51 ( 99.59)	Acc@3 100.00 (100.00)
[2025-02-05 14:36:23,298 INFO] Learning rate: 1.3422170722286599e-06
[2025-02-05 14:36:39,183 INFO] Epoch 48, valid metric: {"Acc@1": 8.103, "Acc@3": 17.728, "loss": 6.711}
[2025-02-05 14:36:46,032 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch45.mdl
[2025-02-05 14:36:48,228 INFO] Epoch: [49][  0/111]	Loss 0.05273 (0.05273)	InvT  22.31 ( 22.31)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2025-02-05 14:37:02,315 INFO] Epoch: [49][ 20/111]	Loss 0.04119 (0.04252)	InvT  22.31 ( 22.31)	Acc@1  99.71 ( 99.63)	Acc@3 100.00 (100.00)
[2025-02-05 14:37:16,454 INFO] Epoch: [49][ 40/111]	Loss 0.04612 (0.04301)	InvT  22.31 ( 22.31)	Acc@1  99.32 ( 99.62)	Acc@3 100.00 (100.00)
[2025-02-05 14:37:30,397 INFO] Epoch: [49][ 60/111]	Loss 0.05365 (0.04303)	InvT  22.31 ( 22.31)	Acc@1  99.22 ( 99.61)	Acc@3 100.00 (100.00)
[2025-02-05 14:37:44,309 INFO] Epoch: [49][ 80/111]	Loss 0.04494 (0.04301)	InvT  22.31 ( 22.31)	Acc@1  99.71 ( 99.63)	Acc@3 100.00 (100.00)
[2025-02-05 14:37:58,205 INFO] Epoch: [49][100/111]	Loss 0.04327 (0.04315)	InvT  22.31 ( 22.31)	Acc@1  99.71 ( 99.62)	Acc@3 100.00 (100.00)
[2025-02-05 14:38:04,985 INFO] Learning rate: 2.7037466203167244e-07
[2025-02-05 14:38:20,593 INFO] Epoch 49, valid metric: {"Acc@1": 8.009, "Acc@3": 17.583, "loss": 6.702}
[2025-02-05 14:38:26,122 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch46.mdl
